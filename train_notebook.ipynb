{
 "cells": [
  {
   "cell_type": "markdown",
   "metadata": {
    "id": "4bsPZQKgE3tX"
   },
   "source": [
    "## AIGCDetectBenchmark\n"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## Install requirement"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "!git clone https://github.com/vohoaidanh/AIGDetection.git"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "colab": {
     "base_uri": "https://localhost:8080/"
    },
    "id": "yZtRWDeMing8",
    "outputId": "5d83d40e-05ac-4bbe-a5b5-29ea56eaaca5",
    "scrolled": true
   },
   "outputs": [],
   "source": [
    "!pip install ftfy -q\n",
    "!pip install natsort -q\n",
    "!pip install blobfile -q\n",
    "#!pip install mpi4py -q\n",
    "!pip install comet_ml -q\n",
    "!pip install grad-cam -q\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "scrolled": true
   },
   "outputs": [],
   "source": [
    "!pip install tensorboardX -q\n",
    "!apt-get install -y unzip -q\n",
    "!apt-get install -y zip -q\n",
    "!pip install regex -q\n",
    "!pip install imageio -q\n",
    "!pip install opencv-python -q\n",
    "!apt-get install -y libgl1-mesa-glx -q\n",
    "!pip install scikit-learn -q\n",
    "!pip install scikit-image -q\n",
    "!pip install gdown -q\n",
    "!pip install pickleshare -q"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "scrolled": true
   },
   "outputs": [],
   "source": [
    "!pip install einops\n",
    "!pip install transformers"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 6,
   "metadata": {
    "colab": {
     "base_uri": "https://localhost:8080/"
    },
    "id": "n9HXSjhchADX",
    "outputId": "e7c10e63-1b0e-4c5f-8ea6-cdc24bd92a72"
   },
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "/workspace/AIGDetection\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "/opt/conda/lib/python3.10/site-packages/IPython/core/magics/osm.py:417: UserWarning: using dhist requires you to install the `pickleshare` library.\n",
      "  self.shell.db['dhist'] = compress_dhist(dhist)[-100:]\n"
     ]
    }
   ],
   "source": [
    "%cd /workspace/AIGDetection"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## Train"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 22,
   "metadata": {},
   "outputs": [],
   "source": [
    "!rm -r checkpoints"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "scrolled": true
   },
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "----------------- Options ---------------\n",
      "                     arch: res50                         \n",
      "               batch_size: 32                            \t[default: 64]\n",
      "                    beta1: 0.9                           \n",
      "                blur_prob: 0                             \n",
      "                 blur_sig: 0.5                           \n",
      "          checkpoints_dir: ./checkpoints                 \n",
      "                class_bal: False                         \n",
      "                  classes: car,cat,chair,horse           \t[default: ]\n",
      "           continue_train: False                         \n",
      "                 cropSize: 224                           \n",
      "                 data_aug: False                         \n",
      "                 dataroot: /workspace/datasets/ForenSynths_train_val\t[default: ./dataset/]\n",
      "                delr_freq: 5                             \t[default: 20]\n",
      "            detect_method: local_grad                    \t[default: NPR]\n",
      "          earlystop_epoch: 15                            \n",
      "                    epoch: latest                        \n",
      "              epoch_count: 1                             \n",
      "                  gpu_ids: 0                             \n",
      "                init_gain: 0.02                          \n",
      "                init_type: normal                        \n",
      "                  isTrain: True                          \t[default: None]\n",
      "               jpg_method: cv2                           \n",
      "                 jpg_prob: 0                             \n",
      "                 jpg_qual: 75                            \n",
      "               last_epoch: -1                            \n",
      "                 loadSize: 256                           \n",
      "                  loss_fn: BCEWithLogitsLoss             \n",
      "                loss_freq: 400                           \n",
      "                       lr: 0.0002                        \t[default: 0.0001]\n",
      "                     mode: binary                        \n",
      "                     name: local-grad-attention-cls-combine-2024_08_17_07_55_01\t[default: experiment_name]\n",
      "                new_optim: False                         \n",
      "                    niter: 30                            \t[default: 1000]\n",
      "                  no_flip: False                         \n",
      "              num_threads: 4                             \t[default: 8]\n",
      "                    optim: adam                          \n",
      "           resize_or_crop: scale_and_crop                \n",
      "                rz_interp: bilinear                      \n",
      "          save_epoch_freq: 20                            \n",
      "         save_latest_freq: 2000                          \n",
      "           serial_batches: False                         \n",
      "                   suffix:                               \n",
      "              train_split: train                         \n",
      "                val_split: val                           \n",
      "----------------- End -------------------\n",
      "train.py  --name  local-grad-attention-cls-combine-  --dataroot  /workspace/datasets/ForenSynths_train_val  --detect_method  local_grad  --num_thread  4  --classes  car,cat,chair,horse  --batch_size  32  --delr_freq  5  --lr  0.0002  --niter  30\n",
      "Detect method model local_grad\n",
      "Downloading: \"https://download.pytorch.org/models/resnet50-19c8e357.pth\" to /root/.cache/torch/hub/checkpoints/resnet50-19c8e357.pth\n",
      "100%|███████████████████████████████████████| 97.8M/97.8M [00:00<00:00, 109MB/s]\n",
      "cwd: /workspace/AIGDetection\n",
      "2024_08_17_07_55_42 Train loss: 0.07191494852304459 at step: 400 lr 0.0002\n",
      "2024_08_17_07_56_22 Train loss: 0.02990874834358692 at step: 800 lr 0.0002\n",
      "2024_08_17_07_57_02 Train loss: 0.01000870019197464 at step: 1200 lr 0.0002\n",
      "2024_08_17_07_57_42 Train loss: 0.18901771306991577 at step: 1600 lr 0.0002\n",
      "2024_08_17_07_58_22 Train loss: 0.006624076049774885 at step: 2000 lr 0.0002\n",
      "2024_08_17_07_59_02 Train loss: 0.006197895854711533 at step: 2400 lr 0.0002\n",
      "2024_08_17_07_59_43 Train loss: 0.005467096343636513 at step: 2800 lr 0.0002\n",
      "2024_08_17_08_00_23 Train loss: 0.05616840347647667 at step: 3200 lr 0.0002\n",
      "2024_08_17_08_01_03 Train loss: 0.032068993896245956 at step: 3600 lr 0.0002\n",
      "2024_08_17_08_01_44 Train loss: 0.03706802800297737 at step: 4000 lr 0.0002\n",
      "2024_08_17_08_02_24 Train loss: 0.013898263685405254 at step: 4400 lr 0.0002\n",
      "(Val @ epoch 0) acc: 0.97625; ap: 0.9998387628247115\n",
      "Saving model ./checkpoints/local-grad-attention-cls-combine-2024_08_17_07_55_01/model_epoch_last.pth\n",
      "acc increate 0 --> 0.97625, saving best model\n",
      "Saving model ./checkpoints/local-grad-attention-cls-combine-2024_08_17_07_55_01/model_epoch_0_best.pth\n",
      "*************************\n",
      "2024_08_17_08_02_36\n",
      "(0 progan      ) acc: 97.8; ap: 100.0; r_acc: 1.0; f_acc: 1.0\n",
      "(1 stylegan    ) acc: 95.7; ap: 99.9; r_acc: 0.9; f_acc: 1.0\n",
      "(2 stylegan2   ) acc: 94.7; ap: 99.4; r_acc: 0.9; f_acc: 1.0\n",
      "(3 biggan      ) acc: 68.2; ap: 81.4; r_acc: 0.4; f_acc: 1.0\n",
      "(4 cyclegan    ) acc: 70.0; ap: 76.2; r_acc: 0.5; f_acc: 0.9\n",
      "(5 stargan     ) acc: 68.0; ap: 97.4; r_acc: 0.4; f_acc: 1.0\n",
      "(6 gaugan      ) acc: 56.8; ap: 61.8; r_acc: 0.2; f_acc: 0.9\n",
      "(7 deepfake    ) acc: 65.2; ap: 90.6; r_acc: 0.3; f_acc: 1.0\n",
      "(8 Mean      ) acc: 77.0; ap: 88.3\n",
      "*************************\n",
      "2024_08_17_08_03_00\n",
      "2024_08_17_08_03_31 Train loss: 0.002398161217570305 at step: 4800 lr 0.0002\n",
      "2024_08_17_08_04_11 Train loss: 0.011869985610246658 at step: 5200 lr 0.0002\n",
      "2024_08_17_08_04_51 Train loss: 0.00020997381943743676 at step: 5600 lr 0.0002\n",
      "2024_08_17_08_05_32 Train loss: 0.002359758596867323 at step: 6000 lr 0.0002\n",
      "2024_08_17_08_06_12 Train loss: 0.0015808900352567434 at step: 6400 lr 0.0002\n",
      "2024_08_17_08_06_52 Train loss: 0.006317674648016691 at step: 6800 lr 0.0002\n",
      "2024_08_17_08_07_33 Train loss: 0.028196096420288086 at step: 7200 lr 0.0002\n",
      "2024_08_17_08_08_13 Train loss: 0.0061290068551898 at step: 7600 lr 0.0002\n",
      "2024_08_17_08_08_53 Train loss: 0.00019977035117335618 at step: 8000 lr 0.0002\n",
      "2024_08_17_08_09_33 Train loss: 0.004900526255369186 at step: 8400 lr 0.0002\n",
      "2024_08_17_08_10_14 Train loss: 0.012433725409209728 at step: 8800 lr 0.0002\n",
      "(Val @ epoch 1) acc: 0.9875; ap: 0.9999489300721238\n",
      "Saving model ./checkpoints/local-grad-attention-cls-combine-2024_08_17_07_55_01/model_epoch_last.pth\n",
      "acc increate 0.97625 --> 0.9875, saving best model\n",
      "Saving model ./checkpoints/local-grad-attention-cls-combine-2024_08_17_07_55_01/model_epoch_1_best.pth\n",
      "*************************\n",
      "2024_08_17_08_10_36\n",
      "(0 progan      ) acc: 97.8; ap: 100.0; r_acc: 1.0; f_acc: 1.0\n",
      "(1 stylegan    ) acc: 88.7; ap: 99.9; r_acc: 1.0; f_acc: 0.8\n",
      "(2 stylegan2   ) acc: 92.9; ap: 99.8; r_acc: 1.0; f_acc: 0.9\n",
      "(3 biggan      ) acc: 67.8; ap: 89.2; r_acc: 1.0; f_acc: 0.4\n",
      "(4 cyclegan    ) acc: 62.9; ap: 95.1; r_acc: 1.0; f_acc: 0.3\n",
      "(5 stargan     ) acc: 70.0; ap: 98.9; r_acc: 1.0; f_acc: 0.4\n",
      "(6 gaugan      ) acc: 57.2; ap: 80.6; r_acc: 1.0; f_acc: 0.2\n",
      "(7 deepfake    ) acc: 50.2; ap: 82.1; r_acc: 1.0; f_acc: 0.0\n",
      "(8 Mean      ) acc: 73.5; ap: 93.2\n",
      "*************************\n",
      "2024_08_17_08_11_00\n",
      "2024_08_17_08_11_20 Train loss: 0.005663284100592136 at step: 9200 lr 0.0002\n",
      "2024_08_17_08_12_00 Train loss: 0.0018292989116162062 at step: 9600 lr 0.0002\n",
      "2024_08_17_08_12_40 Train loss: 0.13133102655410767 at step: 10000 lr 0.0002\n",
      "2024_08_17_08_13_20 Train loss: 0.0040702130645513535 at step: 10400 lr 0.0002\n",
      "2024_08_17_08_14_00 Train loss: 0.00023619687999598682 at step: 10800 lr 0.0002\n",
      "2024_08_17_08_14_40 Train loss: 0.0006657365593127906 at step: 11200 lr 0.0002\n",
      "2024_08_17_08_15_20 Train loss: 0.015028834342956543 at step: 11600 lr 0.0002\n",
      "2024_08_17_08_16_00 Train loss: 0.03907343000173569 at step: 12000 lr 0.0002\n",
      "2024_08_17_08_16_53 Train loss: 0.0002849611919373274 at step: 12400 lr 0.0002\n",
      "2024_08_17_08_17_48 Train loss: 0.007133071776479483 at step: 12800 lr 0.0002\n",
      "2024_08_17_08_18_29 Train loss: 0.0014318436151370406 at step: 13200 lr 0.0002\n",
      "(Val @ epoch 2) acc: 0.99875; ap: 0.9999906947890819\n",
      "Saving model ./checkpoints/local-grad-attention-cls-combine-2024_08_17_07_55_01/model_epoch_last.pth\n",
      "acc increate 0.9875 --> 0.99875, saving best model\n",
      "Saving model ./checkpoints/local-grad-attention-cls-combine-2024_08_17_07_55_01/model_epoch_2_best.pth\n",
      "*************************\n",
      "2024_08_17_08_19_01\n",
      "(0 progan      ) acc: 99.6; ap: 100.0; r_acc: 1.0; f_acc: 1.0\n",
      "(1 stylegan    ) acc: 99.5; ap: 100.0; r_acc: 1.0; f_acc: 1.0\n",
      "(2 stylegan2   ) acc: 97.2; ap: 99.9; r_acc: 1.0; f_acc: 0.9\n",
      "(3 biggan      ) acc: 78.8; ap: 89.4; r_acc: 0.6; f_acc: 0.9\n",
      "(4 cyclegan    ) acc: 83.5; ap: 93.5; r_acc: 0.9; f_acc: 0.8\n",
      "(5 stargan     ) acc: 98.2; ap: 99.9; r_acc: 1.0; f_acc: 1.0\n",
      "(6 gaugan      ) acc: 72.0; ap: 77.6; r_acc: 0.5; f_acc: 0.9\n",
      "(7 deepfake    ) acc: 81.2; ap: 92.3; r_acc: 0.9; f_acc: 0.7\n",
      "(8 Mean      ) acc: 88.8; ap: 94.1\n",
      "*************************\n",
      "2024_08_17_08_19_25\n",
      "2024_08_17_08_19_35 Train loss: 0.017687629908323288 at step: 13600 lr 0.0002\n",
      "2024_08_17_08_20_15 Train loss: 0.001752877957187593 at step: 14000 lr 0.0002\n",
      "2024_08_17_08_20_55 Train loss: 0.000781162700150162 at step: 14400 lr 0.0002\n",
      "2024_08_17_08_21_36 Train loss: 0.0003003791789524257 at step: 14800 lr 0.0002\n",
      "2024_08_17_08_22_16 Train loss: 0.0014091794146224856 at step: 15200 lr 0.0002\n",
      "2024_08_17_08_22_56 Train loss: 0.012308848090469837 at step: 15600 lr 0.0002\n",
      "2024_08_17_08_23_36 Train loss: 0.0014101703418418765 at step: 16000 lr 0.0002\n",
      "2024_08_17_08_24_16 Train loss: 0.004214344546198845 at step: 16400 lr 0.0002\n",
      "2024_08_17_08_24_57 Train loss: 0.0002939075930044055 at step: 16800 lr 0.0002\n",
      "2024_08_17_08_25_37 Train loss: 4.1487157432129607e-05 at step: 17200 lr 0.0002\n",
      "2024_08_17_08_26_22 Train loss: 0.010733300819993019 at step: 17600 lr 0.0002\n",
      "2024_08_17_08_27_27 Train loss: 0.0016817059367895126 at step: 18000 lr 0.0002\n",
      "(Val @ epoch 3) acc: 0.97375; ap: 0.9999830054012345\n",
      "Saving model ./checkpoints/local-grad-attention-cls-combine-2024_08_17_07_55_01/model_epoch_last.pth\n",
      "Saving model ./checkpoints/local-grad-attention-cls-combine-2024_08_17_07_55_01/model_epoch_3.pth\n",
      "early stop count 0/15\n",
      "*************************\n",
      "2024_08_17_08_27_31\n",
      "(0 progan      ) acc: 97.4; ap: 100.0; r_acc: 0.9; f_acc: 1.0\n",
      "(1 stylegan    ) acc: 97.3; ap: 100.0; r_acc: 0.9; f_acc: 1.0\n",
      "(2 stylegan2   ) acc: 96.5; ap: 99.9; r_acc: 0.9; f_acc: 1.0\n",
      "(3 biggan      ) acc: 71.2; ap: 86.5; r_acc: 0.5; f_acc: 1.0\n",
      "(4 cyclegan    ) acc: 74.6; ap: 85.1; r_acc: 0.7; f_acc: 0.8\n",
      "(5 stargan     ) acc: 80.0; ap: 99.0; r_acc: 0.6; f_acc: 1.0\n",
      "(6 gaugan      ) acc: 64.2; ap: 70.5; r_acc: 0.3; f_acc: 0.9\n",
      "(7 deepfake    ) acc: 72.5; ap: 92.6; r_acc: 0.5; f_acc: 1.0\n",
      "(8 Mean      ) acc: 81.7; ap: 91.7\n",
      "*************************\n",
      "2024_08_17_08_28_06\n",
      "2024_08_17_08_29_10 Train loss: 0.0014985328307375312 at step: 18400 lr 0.0002\n",
      "2024_08_17_08_30_15 Train loss: 0.000744309276342392 at step: 18800 lr 0.0002\n",
      "2024_08_17_08_31_22 Train loss: 0.00012091951066395268 at step: 19200 lr 0.0002\n",
      "2024_08_17_08_32_02 Train loss: 0.0002911621704697609 at step: 19600 lr 0.0002\n",
      "2024_08_17_08_32_42 Train loss: 0.02904350683093071 at step: 20000 lr 0.0002\n",
      "2024_08_17_08_33_22 Train loss: 0.0003938513982575387 at step: 20400 lr 0.0002\n",
      "2024_08_17_08_34_03 Train loss: 0.017295151948928833 at step: 20800 lr 0.0002\n",
      "2024_08_17_08_34_43 Train loss: 0.0003964517673011869 at step: 21200 lr 0.0002\n",
      "2024_08_17_08_35_23 Train loss: 0.003787684952840209 at step: 21600 lr 0.0002\n",
      "2024_08_17_08_36_04 Train loss: 0.00012062445603078231 at step: 22000 lr 0.0002\n",
      "2024_08_17_08_36_44 Train loss: 0.0012063469039276242 at step: 22400 lr 0.0002\n",
      "(Val @ epoch 4) acc: 0.9975; ap: 0.9999968827930175\n",
      "Saving model ./checkpoints/local-grad-attention-cls-combine-2024_08_17_07_55_01/model_epoch_last.pth\n",
      "Saving model ./checkpoints/local-grad-attention-cls-combine-2024_08_17_07_55_01/model_epoch_4.pth\n",
      "early stop count 0/15\n",
      "*************************\n",
      "2024_08_17_08_36_57\n",
      "(0 progan      ) acc: 99.5; ap: 100.0; r_acc: 1.0; f_acc: 1.0\n",
      "(1 stylegan    ) acc: 99.7; ap: 100.0; r_acc: 1.0; f_acc: 1.0\n",
      "(2 stylegan2   ) acc: 98.6; ap: 99.9; r_acc: 1.0; f_acc: 1.0\n",
      "(3 biggan      ) acc: 75.0; ap: 89.5; r_acc: 0.5; f_acc: 1.0\n",
      "(4 cyclegan    ) acc: 80.5; ap: 91.1; r_acc: 0.8; f_acc: 0.8\n",
      "(5 stargan     ) acc: 96.2; ap: 100.0; r_acc: 0.9; f_acc: 1.0\n",
      "(6 gaugan      ) acc: 66.0; ap: 71.7; r_acc: 0.4; f_acc: 0.9\n",
      "(7 deepfake    ) acc: 84.8; ap: 93.5; r_acc: 0.8; f_acc: 0.9\n",
      "(8 Mean      ) acc: 87.5; ap: 93.2\n",
      "*************************\n",
      "2024_08_17_08_37_21\n",
      "2024_08_17_08_37_50 Train loss: 0.00022687582531943917 at step: 22800 lr 0.0002\n",
      "2024_08_17_08_38_52 Train loss: 2.6162000722251832e-05 at step: 23200 lr 0.0002\n",
      "2024_08_17_08_39_57 Train loss: 0.004501441493630409 at step: 23600 lr 0.0002\n",
      "2024_08_17_08_41_08 Train loss: 0.0022200022358447313 at step: 24000 lr 0.0002\n",
      "2024_08_17_08_42_14 Train loss: 4.085096588823944e-05 at step: 24400 lr 0.0002\n",
      "2024_08_17_08_43_20 Train loss: 6.180078344186768e-06 at step: 24800 lr 0.0002\n",
      "2024_08_17_08_44_13 Train loss: 0.35123634338378906 at step: 25200 lr 0.0002\n",
      "2024_08_17_08_45_03 Train loss: 5.634931949316524e-05 at step: 25600 lr 0.0002\n",
      "2024_08_17_08_46_06 Train loss: 0.00439633009955287 at step: 26000 lr 0.0002\n",
      "2024_08_17_08_46_47 Train loss: 0.000112150824861601 at step: 26400 lr 0.0002\n",
      "2024_08_17_08_47_27 Train loss: 0.00045274264994077384 at step: 26800 lr 0.0002\n",
      "2024_08_17_08_47_48 changing lr at the end of epoch 5, iters 27006\n",
      "*************************\n",
      "Changing lr from 0.0002 to 0.00018\n",
      "*************************\n",
      "(Val @ epoch 5) acc: 0.955625; ap: 0.999885659020145\n",
      "Saving model ./checkpoints/local-grad-attention-cls-combine-2024_08_17_07_55_01/model_epoch_last.pth\n",
      "Saving model ./checkpoints/local-grad-attention-cls-combine-2024_08_17_07_55_01/model_epoch_5.pth\n",
      "early stop count 0/15\n",
      "*************************\n",
      "2024_08_17_08_47_50\n",
      "(0 progan      ) acc: 95.0; ap: 99.9; r_acc: 1.0; f_acc: 0.9\n",
      "(1 stylegan    ) acc: 92.1; ap: 99.8; r_acc: 1.0; f_acc: 0.8\n",
      "(2 stylegan2   ) acc: 91.1; ap: 99.4; r_acc: 1.0; f_acc: 0.8\n",
      "(3 biggan      ) acc: 79.2; ap: 89.0; r_acc: 0.9; f_acc: 0.7\n",
      "(4 cyclegan    ) acc: 71.8; ap: 95.5; r_acc: 1.0; f_acc: 0.5\n",
      "(5 stargan     ) acc: 97.0; ap: 99.9; r_acc: 1.0; f_acc: 0.9\n",
      "(6 gaugan      ) acc: 67.5; ap: 80.2; r_acc: 0.8; f_acc: 0.6\n",
      "(7 deepfake    ) acc: 86.0; ap: 94.7; r_acc: 1.0; f_acc: 0.8\n",
      "(8 Mean      ) acc: 85.0; ap: 94.8\n",
      "*************************\n",
      "2024_08_17_08_48_14\n",
      "2024_08_17_08_48_34 Train loss: 7.504129462176934e-05 at step: 27200 lr 0.00018\n",
      "2024_08_17_08_49_14 Train loss: 0.0065386551432311535 at step: 27600 lr 0.00018\n",
      "2024_08_17_08_49_54 Train loss: 1.539829099783674e-05 at step: 28000 lr 0.00018\n",
      "2024_08_17_08_50_35 Train loss: 9.949345439963508e-06 at step: 28400 lr 0.00018\n",
      "2024_08_17_08_51_15 Train loss: 0.023502789437770844 at step: 28800 lr 0.00018\n",
      "2024_08_17_08_51_55 Train loss: 0.0003420499269850552 at step: 29200 lr 0.00018\n",
      "2024_08_17_08_52_35 Train loss: 0.0004225915181450546 at step: 29600 lr 0.00018\n",
      "2024_08_17_08_53_16 Train loss: 0.0003045275807380676 at step: 30000 lr 0.00018\n",
      "2024_08_17_08_54_04 Train loss: 4.750328662339598e-05 at step: 30400 lr 0.00018\n",
      "2024_08_17_08_55_06 Train loss: 1.288937482968322e-06 at step: 30800 lr 0.00018\n",
      "2024_08_17_08_56_16 Train loss: 1.9498702386044897e-05 at step: 31200 lr 0.00018\n",
      "(Val @ epoch 6) acc: 0.99375; ap: 1.0\n",
      "Saving model ./checkpoints/local-grad-attention-cls-combine-2024_08_17_07_55_01/model_epoch_last.pth\n",
      "Saving model ./checkpoints/local-grad-attention-cls-combine-2024_08_17_07_55_01/model_epoch_6.pth\n",
      "early stop count 0/15\n",
      "*************************\n",
      "2024_08_17_08_57_14\n",
      "(0 progan      ) acc: 98.8; ap: 100.0; r_acc: 1.0; f_acc: 1.0\n",
      "(1 stylegan    ) acc: 98.0; ap: 100.0; r_acc: 1.0; f_acc: 1.0\n",
      "(2 stylegan2   ) acc: 94.7; ap: 99.9; r_acc: 1.0; f_acc: 0.9\n",
      "(3 biggan      ) acc: 84.8; ap: 92.0; r_acc: 0.8; f_acc: 0.9\n",
      "(4 cyclegan    ) acc: 81.7; ap: 97.1; r_acc: 1.0; f_acc: 0.7\n",
      "(5 stargan     ) acc: 98.2; ap: 100.0; r_acc: 1.0; f_acc: 1.0\n",
      "(6 gaugan      ) acc: 72.0; ap: 77.2; r_acc: 0.7; f_acc: 0.7\n",
      "(7 deepfake    ) acc: 77.5; ap: 90.0; r_acc: 0.9; f_acc: 0.6\n",
      "(8 Mean      ) acc: 88.2; ap: 94.5\n",
      "*************************\n",
      "2024_08_17_08_57_48\n",
      "2024_08_17_08_58_02 Train loss: 0.0008054298814386129 at step: 31600 lr 0.00018\n",
      "2024_08_17_08_59_09 Train loss: 1.144262478192104e-05 at step: 32000 lr 0.00018\n",
      "2024_08_17_08_59_56 Train loss: 0.0001373733684886247 at step: 32400 lr 0.00018\n",
      "2024_08_17_09_01_00 Train loss: 0.00030022853752598166 at step: 32800 lr 0.00018\n",
      "2024_08_17_09_02_05 Train loss: 2.5396460841875523e-05 at step: 33200 lr 0.00018\n",
      "2024_08_17_09_03_16 Train loss: 0.00017049050075002015 at step: 33600 lr 0.00018\n",
      "2024_08_17_09_04_21 Train loss: 0.0007845808286219835 at step: 34000 lr 0.00018\n",
      "2024_08_17_09_05_27 Train loss: 0.0006061490275897086 at step: 34400 lr 0.00018\n",
      "2024_08_17_09_06_19 Train loss: 0.00010017292515840381 at step: 34800 lr 0.00018\n",
      "2024_08_17_09_07_09 Train loss: 0.04282696545124054 at step: 35200 lr 0.00018\n",
      "2024_08_17_09_08_11 Train loss: 0.00730173010379076 at step: 35600 lr 0.00018\n",
      "2024_08_17_09_08_51 Train loss: 0.0024019062984734774 at step: 36000 lr 0.00018\n",
      "(Val @ epoch 7) acc: 0.999375; ap: 0.9999984394506867\n",
      "Saving model ./checkpoints/local-grad-attention-cls-combine-2024_08_17_07_55_01/model_epoch_last.pth\n",
      "acc increate 0.99875 --> 0.999375, saving best model\n",
      "Saving model ./checkpoints/local-grad-attention-cls-combine-2024_08_17_07_55_01/model_epoch_7_best.pth\n",
      "*************************\n",
      "2024_08_17_09_08_54\n",
      "(0 progan      ) acc: 99.8; ap: 100.0; r_acc: 1.0; f_acc: 1.0\n",
      "(1 stylegan    ) acc: 99.0; ap: 100.0; r_acc: 1.0; f_acc: 1.0\n",
      "(2 stylegan2   ) acc: 98.5; ap: 100.0; r_acc: 1.0; f_acc: 1.0\n",
      "(3 biggan      ) acc: 86.5; ap: 92.5; r_acc: 0.8; f_acc: 0.9\n",
      "(4 cyclegan    ) acc: 75.1; ap: 92.5; r_acc: 0.9; f_acc: 0.6\n",
      "(5 stargan     ) acc: 91.0; ap: 98.4; r_acc: 1.0; f_acc: 0.9\n",
      "(6 gaugan      ) acc: 69.0; ap: 70.8; r_acc: 0.7; f_acc: 0.7\n",
      "(7 deepfake    ) acc: 62.5; ap: 85.9; r_acc: 1.0; f_acc: 0.3\n",
      "(8 Mean      ) acc: 85.2; ap: 92.5\n",
      "*************************\n",
      "2024_08_17_09_09_19\n",
      "2024_08_17_09_09_58 Train loss: 0.00045543923624791205 at step: 36400 lr 0.00018\n",
      "2024_08_17_09_10_38 Train loss: 2.5540981368976645e-05 at step: 36800 lr 0.00018\n",
      "2024_08_17_09_11_19 Train loss: 0.0007598440279252827 at step: 37200 lr 0.00018\n",
      "2024_08_17_09_12_02 Train loss: 0.12174301594495773 at step: 37600 lr 0.00018\n",
      "2024_08_17_09_13_05 Train loss: 0.0001871832791948691 at step: 38000 lr 0.00018\n",
      "2024_08_17_09_14_10 Train loss: 5.80428859393578e-05 at step: 38400 lr 0.00018\n",
      "2024_08_17_09_15_20 Train loss: 9.088905244425405e-06 at step: 38800 lr 0.00018\n",
      "2024_08_17_09_16_25 Train loss: 0.00013089344429317862 at step: 39200 lr 0.00018\n",
      "2024_08_17_09_17_30 Train loss: 0.0012444203021004796 at step: 39600 lr 0.00018\n",
      "2024_08_17_09_18_22 Train loss: 0.00015383936988655478 at step: 40000 lr 0.00018\n",
      "2024_08_17_09_19_11 Train loss: 0.00017239348380826414 at step: 40400 lr 0.00018\n",
      "(Val @ epoch 8) acc: 0.994375; ap: 0.9999875504706486\n",
      "Saving model ./checkpoints/local-grad-attention-cls-combine-2024_08_17_07_55_01/model_epoch_last.pth\n",
      "Saving model ./checkpoints/local-grad-attention-cls-combine-2024_08_17_07_55_01/model_epoch_8.pth\n",
      "early stop count 0/15\n",
      "*************************\n",
      "2024_08_17_09_19_33\n",
      "(0 progan      ) acc: 98.8; ap: 100.0; r_acc: 1.0; f_acc: 1.0\n",
      "(1 stylegan    ) acc: 98.3; ap: 100.0; r_acc: 1.0; f_acc: 1.0\n",
      "(2 stylegan2   ) acc: 96.2; ap: 100.0; r_acc: 1.0; f_acc: 0.9\n",
      "(3 biggan      ) acc: 85.8; ap: 92.3; r_acc: 0.9; f_acc: 0.8\n",
      "(4 cyclegan    ) acc: 77.8; ap: 96.9; r_acc: 1.0; f_acc: 0.6\n",
      "(5 stargan     ) acc: 97.0; ap: 100.0; r_acc: 1.0; f_acc: 0.9\n",
      "(6 gaugan      ) acc: 77.2; ap: 82.0; r_acc: 0.8; f_acc: 0.7\n",
      "(7 deepfake    ) acc: 55.2; ap: 89.5; r_acc: 1.0; f_acc: 0.1\n",
      "(8 Mean      ) acc: 85.8; ap: 95.1\n",
      "*************************\n",
      "2024_08_17_09_20_06\n",
      "2024_08_17_09_20_36 Train loss: 4.4736261770594865e-05 at step: 40800 lr 0.00018\n",
      "2024_08_17_09_21_16 Train loss: 0.001179134356789291 at step: 41200 lr 0.00018\n",
      "2024_08_17_09_21_56 Train loss: 5.709337710868567e-05 at step: 41600 lr 0.00018\n",
      "2024_08_17_09_22_36 Train loss: 3.755444777198136e-05 at step: 42000 lr 0.00018\n",
      "2024_08_17_09_23_17 Train loss: 0.0001816818694351241 at step: 42400 lr 0.00018\n",
      "2024_08_17_09_23_57 Train loss: 3.7102877286088187e-06 at step: 42800 lr 0.00018\n",
      "2024_08_17_09_24_37 Train loss: 6.883857622597134e-06 at step: 43200 lr 0.00018\n",
      "2024_08_17_09_25_18 Train loss: 0.021647606045007706 at step: 43600 lr 0.00018\n",
      "2024_08_17_09_25_58 Train loss: 0.001010364037938416 at step: 44000 lr 0.00018\n",
      "2024_08_17_09_26_38 Train loss: 1.322471689491067e-06 at step: 44400 lr 0.00018\n",
      "2024_08_17_09_27_18 Train loss: 8.060496838879772e-06 at step: 44800 lr 0.00018\n",
      "(Val @ epoch 9) acc: 0.9975; ap: 0.9999890839138266\n",
      "Saving model ./checkpoints/local-grad-attention-cls-combine-2024_08_17_07_55_01/model_epoch_last.pth\n",
      "Saving model ./checkpoints/local-grad-attention-cls-combine-2024_08_17_07_55_01/model_epoch_9.pth\n",
      "early stop count 0/15\n",
      "*************************\n",
      "2024_08_17_09_27_42\n",
      "(0 progan      ) acc: 99.8; ap: 100.0; r_acc: 1.0; f_acc: 1.0\n",
      "(1 stylegan    ) acc: 99.8; ap: 100.0; r_acc: 1.0; f_acc: 1.0\n",
      "(2 stylegan2   ) acc: 99.4; ap: 100.0; r_acc: 1.0; f_acc: 1.0\n",
      "(3 biggan      ) acc: 80.0; ap: 91.5; r_acc: 0.6; f_acc: 1.0\n",
      "(4 cyclegan    ) acc: 85.6; ap: 94.1; r_acc: 0.9; f_acc: 0.8\n",
      "(5 stargan     ) acc: 96.8; ap: 100.0; r_acc: 0.9; f_acc: 1.0\n",
      "(6 gaugan      ) acc: 71.0; ap: 81.4; r_acc: 0.5; f_acc: 0.9\n",
      "(7 deepfake    ) acc: 85.0; ap: 92.8; r_acc: 0.9; f_acc: 0.8\n",
      "(8 Mean      ) acc: 89.7; ap: 95.0\n",
      "*************************\n",
      "2024_08_17_09_28_06\n",
      "2024_08_17_09_28_25 Train loss: 2.945443156932015e-05 at step: 45200 lr 0.00018\n",
      "2024_08_17_09_29_05 Train loss: 5.7699198805494234e-05 at step: 45600 lr 0.00018\n",
      "2024_08_17_09_29_46 Train loss: 1.1920921139108032e-07 at step: 46000 lr 0.00018\n",
      "2024_08_17_09_30_26 Train loss: 0.00047771527897566557 at step: 46400 lr 0.00018\n",
      "2024_08_17_09_31_06 Train loss: 4.280179837223841e-06 at step: 46800 lr 0.00018\n",
      "2024_08_17_09_31_47 Train loss: 4.071609964739764e-06 at step: 47200 lr 0.00018\n",
      "2024_08_17_09_32_27 Train loss: 6.455810216721147e-05 at step: 47600 lr 0.00018\n",
      "2024_08_17_09_33_07 Train loss: 2.2903992430656217e-05 at step: 48000 lr 0.00018\n",
      "2024_08_17_09_33_48 Train loss: 6.856690015411004e-05 at step: 48400 lr 0.00018\n",
      "2024_08_17_09_34_28 Train loss: 0.003921492490917444 at step: 48800 lr 0.00018\n",
      "2024_08_17_09_35_08 Train loss: 4.6862483031873126e-06 at step: 49200 lr 0.00018\n",
      "2024_08_17_09_35_40 changing lr at the end of epoch 10, iters 49511\n",
      "*************************\n",
      "Changing lr from 0.00018 to 0.000162\n",
      "*************************\n",
      "(Val @ epoch 10) acc: 0.899375; ap: 0.9992186431335426\n",
      "Saving model ./checkpoints/local-grad-attention-cls-combine-2024_08_17_07_55_01/model_epoch_last.pth\n",
      "Saving model ./checkpoints/local-grad-attention-cls-combine-2024_08_17_07_55_01/model_epoch_10.pth\n",
      "early stop count 0/15\n",
      "*************************\n",
      "2024_08_17_09_35_42\n",
      "(0 progan      ) acc: 89.5; ap: 99.9; r_acc: 1.0; f_acc: 0.8\n",
      "(1 stylegan    ) acc: 86.5; ap: 99.9; r_acc: 1.0; f_acc: 0.7\n",
      "(2 stylegan2   ) acc: 82.8; ap: 99.7; r_acc: 1.0; f_acc: 0.7\n",
      "(3 biggan      ) acc: 79.0; ap: 94.0; r_acc: 1.0; f_acc: 0.6\n",
      "(4 cyclegan    ) acc: 68.0; ap: 94.3; r_acc: 1.0; f_acc: 0.4\n",
      "(5 stargan     ) acc: 92.8; ap: 100.0; r_acc: 1.0; f_acc: 0.9\n",
      "(6 gaugan      ) acc: 66.8; ap: 85.8; r_acc: 1.0; f_acc: 0.4\n",
      "(7 deepfake    ) acc: 51.5; ap: 91.0; r_acc: 1.0; f_acc: 0.0\n",
      "(8 Mean      ) acc: 77.1; ap: 95.6\n",
      "*************************\n",
      "2024_08_17_09_36_06\n",
      "2024_08_17_09_36_15 Train loss: 5.1554516176111065e-06 at step: 49600 lr 0.000162\n",
      "2024_08_17_09_36_55 Train loss: 5.339368726708926e-05 at step: 50000 lr 0.000162\n",
      "2024_08_17_09_37_36 Train loss: 0.0019068242982029915 at step: 50400 lr 0.000162\n",
      "2024_08_17_09_38_16 Train loss: 8.232807431340916e-07 at step: 50800 lr 0.000162\n",
      "2024_08_17_09_38_56 Train loss: 0.001133487792685628 at step: 51200 lr 0.000162\n",
      "2024_08_17_09_39_36 Train loss: 2.0116544874326792e-07 at step: 51600 lr 0.000162\n",
      "2024_08_17_09_40_17 Train loss: 5.587933671336032e-08 at step: 52000 lr 0.000162\n",
      "2024_08_17_09_40_57 Train loss: 1.352271738142008e-06 at step: 52400 lr 0.000162\n",
      "2024_08_17_09_41_37 Train loss: 8.444103877991438e-05 at step: 52800 lr 0.000162\n",
      "2024_08_17_09_42_17 Train loss: 4.678660388890421e-06 at step: 53200 lr 0.000162\n",
      "2024_08_17_09_42_58 Train loss: 0.00028326973551884294 at step: 53600 lr 0.000162\n",
      "2024_08_17_09_43_38 Train loss: 0.006281638517975807 at step: 54000 lr 0.000162\n",
      "(Val @ epoch 11) acc: 0.99875; ap: 1.0\n",
      "Saving model ./checkpoints/local-grad-attention-cls-combine-2024_08_17_07_55_01/model_epoch_last.pth\n",
      "Saving model ./checkpoints/local-grad-attention-cls-combine-2024_08_17_07_55_01/model_epoch_11.pth\n",
      "early stop count 0/15\n",
      "*************************\n",
      "2024_08_17_09_43_42\n",
      "(0 progan      ) acc: 99.6; ap: 100.0; r_acc: 1.0; f_acc: 1.0\n",
      "(1 stylegan    ) acc: 98.3; ap: 100.0; r_acc: 1.0; f_acc: 1.0\n",
      "(2 stylegan2   ) acc: 97.8; ap: 100.0; r_acc: 1.0; f_acc: 1.0\n",
      "(3 biggan      ) acc: 85.5; ap: 94.3; r_acc: 0.9; f_acc: 0.8\n",
      "(4 cyclegan    ) acc: 74.4; ap: 94.9; r_acc: 1.0; f_acc: 0.5\n",
      "(5 stargan     ) acc: 96.8; ap: 99.9; r_acc: 1.0; f_acc: 0.9\n",
      "(6 gaugan      ) acc: 69.5; ap: 81.6; r_acc: 0.8; f_acc: 0.6\n",
      "(7 deepfake    ) acc: 74.5; ap: 90.8; r_acc: 0.9; f_acc: 0.5\n",
      "(8 Mean      ) acc: 87.0; ap: 95.2\n",
      "*************************\n",
      "2024_08_17_09_44_06\n",
      "2024_08_17_09_44_45 Train loss: 0.00011758707842091098 at step: 54400 lr 0.000162\n",
      "2024_08_17_09_45_25 Train loss: 0.001975106308236718 at step: 54800 lr 0.000162\n",
      "2024_08_17_09_46_05 Train loss: 0.0002034045464824885 at step: 55200 lr 0.000162\n",
      "2024_08_17_09_46_45 Train loss: 1.0355365702707786e-05 at step: 55600 lr 0.000162\n",
      "2024_08_17_09_47_26 Train loss: 0.00406344560906291 at step: 56000 lr 0.000162\n",
      "2024_08_17_09_48_06 Train loss: 8.917710147215985e-06 at step: 56400 lr 0.000162\n",
      "2024_08_17_09_48_46 Train loss: 2.607702853651972e-08 at step: 56800 lr 0.000162\n",
      "2024_08_17_09_49_27 Train loss: 3.2147877391253132e-06 at step: 57200 lr 0.000162\n",
      "2024_08_17_09_50_07 Train loss: 7.949261998874135e-06 at step: 57600 lr 0.000162\n",
      "2024_08_17_09_50_47 Train loss: 4.914610690320842e-05 at step: 58000 lr 0.000162\n",
      "2024_08_17_09_51_27 Train loss: 2.241277979919687e-05 at step: 58400 lr 0.000162\n",
      "(Val @ epoch 12) acc: 0.97375; ap: 0.9999723690974517\n",
      "Saving model ./checkpoints/local-grad-attention-cls-combine-2024_08_17_07_55_01/model_epoch_last.pth\n",
      "Saving model ./checkpoints/local-grad-attention-cls-combine-2024_08_17_07_55_01/model_epoch_12.pth\n",
      "early stop count 0/15\n",
      "*************************\n",
      "2024_08_17_09_51_41\n",
      "(0 progan      ) acc: 97.9; ap: 100.0; r_acc: 1.0; f_acc: 1.0\n",
      "(1 stylegan    ) acc: 98.3; ap: 100.0; r_acc: 1.0; f_acc: 1.0\n",
      "(2 stylegan2   ) acc: 97.8; ap: 100.0; r_acc: 1.0; f_acc: 1.0\n",
      "(3 biggan      ) acc: 75.5; ap: 90.5; r_acc: 0.5; f_acc: 1.0\n",
      "(4 cyclegan    ) acc: 76.4; ap: 87.4; r_acc: 0.8; f_acc: 0.7\n",
      "(5 stargan     ) acc: 88.8; ap: 99.6; r_acc: 0.8; f_acc: 1.0\n",
      "(6 gaugan      ) acc: 66.2; ap: 66.2; r_acc: 0.4; f_acc: 0.9\n",
      "(7 deepfake    ) acc: 79.5; ap: 90.3; r_acc: 0.9; f_acc: 0.7\n",
      "(8 Mean      ) acc: 85.0; ap: 91.7\n",
      "*************************\n",
      "2024_08_17_09_52_05\n",
      "2024_08_17_09_52_34 Train loss: 1.989280917769065e-06 at step: 58800 lr 0.000162\n",
      "2024_08_17_09_53_14 Train loss: 3.613421540649142e-06 at step: 59200 lr 0.000162\n",
      "2024_08_17_09_53_54 Train loss: 1.5273661801984417e-07 at step: 59600 lr 0.000162\n",
      "2024_08_17_09_54_35 Train loss: 1.5645840676370426e-06 at step: 60000 lr 0.000162\n",
      "2024_08_17_09_55_15 Train loss: 2.5070801257243147e-06 at step: 60400 lr 0.000162\n",
      "2024_08_17_09_55_55 Train loss: 8.168515705619939e-06 at step: 60800 lr 0.000162\n",
      "2024_08_17_09_56_35 Train loss: 0.00011247614747844636 at step: 61200 lr 0.000162\n",
      "2024_08_17_09_57_23 Train loss: 1.8512158931116574e-05 at step: 61600 lr 0.000162\n",
      "2024_08_17_09_58_25 Train loss: 0.00014818509225733578 at step: 62000 lr 0.000162\n",
      "2024_08_17_09_59_35 Train loss: 4.4703469370688254e-08 at step: 62400 lr 0.000162\n",
      "2024_08_17_10_00_44 Train loss: 0.00016020690964069217 at step: 62800 lr 0.000162\n",
      "(Val @ epoch 13) acc: 0.999375; ap: 1.0\n",
      "Saving model ./checkpoints/local-grad-attention-cls-combine-2024_08_17_07_55_01/model_epoch_last.pth\n",
      "acc increate 0.999375 --> 0.999375, saving best model\n",
      "Saving model ./checkpoints/local-grad-attention-cls-combine-2024_08_17_07_55_01/model_epoch_13_best.pth\n",
      "*************************\n",
      "2024_08_17_10_01_23\n",
      "(0 progan      ) acc: 99.6; ap: 100.0; r_acc: 1.0; f_acc: 1.0\n",
      "(1 stylegan    ) acc: 97.7; ap: 100.0; r_acc: 1.0; f_acc: 1.0\n",
      "(2 stylegan2   ) acc: 97.5; ap: 100.0; r_acc: 1.0; f_acc: 0.9\n",
      "(3 biggan      ) acc: 79.0; ap: 92.9; r_acc: 0.9; f_acc: 0.6\n",
      "(4 cyclegan    ) acc: 72.8; ap: 95.9; r_acc: 1.0; f_acc: 0.5\n",
      "(5 stargan     ) acc: 94.0; ap: 99.9; r_acc: 1.0; f_acc: 0.9\n",
      "(6 gaugan      ) acc: 60.2; ap: 77.2; r_acc: 0.9; f_acc: 0.3\n",
      "(7 deepfake    ) acc: 66.0; ap: 87.1; r_acc: 1.0; f_acc: 0.3\n",
      "(8 Mean      ) acc: 83.4; ap: 94.1\n",
      "*************************\n",
      "2024_08_17_10_01_55\n",
      "2024_08_17_10_02_27 Train loss: 3.121717327303486e-06 at step: 63200 lr 0.000162\n",
      "2024_08_17_10_03_20 Train loss: 0.0003022095770575106 at step: 63600 lr 0.000162\n",
      "2024_08_17_10_04_10 Train loss: 0.00955884251743555 at step: 64000 lr 0.000162\n",
      "2024_08_17_10_05_13 Train loss: 1.862644793959589e-08 at step: 64400 lr 0.000162\n",
      "2024_08_17_10_05_53 Train loss: 0.00031529192347079515 at step: 64800 lr 0.000162\n",
      "2024_08_17_10_06_33 Train loss: 0.0002801580703817308 at step: 65200 lr 0.000162\n",
      "2024_08_17_10_07_13 Train loss: 2.454914920235751e-06 at step: 65600 lr 0.000162\n",
      "2024_08_17_10_07_54 Train loss: 5.829586370964535e-06 at step: 66000 lr 0.000162\n",
      "2024_08_17_10_08_50 Train loss: 0.0024929060600697994 at step: 66400 lr 0.000162\n",
      "2024_08_17_10_09_55 Train loss: 0.00016338429122697562 at step: 66800 lr 0.000162\n",
      "2024_08_17_10_11_06 Train loss: 0.012324600480496883 at step: 67200 lr 0.000162\n",
      "(Val @ epoch 14) acc: 0.999375; ap: 1.0\n",
      "Saving model ./checkpoints/local-grad-attention-cls-combine-2024_08_17_07_55_01/model_epoch_last.pth\n",
      "acc increate 0.999375 --> 0.999375, saving best model\n",
      "Saving model ./checkpoints/local-grad-attention-cls-combine-2024_08_17_07_55_01/model_epoch_14_best.pth\n",
      "*************************\n",
      "2024_08_17_10_12_01\n",
      "(0 progan      ) acc: 99.9; ap: 100.0; r_acc: 1.0; f_acc: 1.0\n",
      "(1 stylegan    ) acc: 99.2; ap: 100.0; r_acc: 1.0; f_acc: 1.0\n",
      "(2 stylegan2   ) acc: 98.5; ap: 100.0; r_acc: 1.0; f_acc: 1.0\n",
      "(3 biggan      ) acc: 86.8; ap: 94.6; r_acc: 0.8; f_acc: 0.9\n",
      "(4 cyclegan    ) acc: 78.5; ap: 96.0; r_acc: 1.0; f_acc: 0.6\n",
      "(5 stargan     ) acc: 99.5; ap: 100.0; r_acc: 1.0; f_acc: 1.0\n",
      "(6 gaugan      ) acc: 72.0; ap: 78.2; r_acc: 0.7; f_acc: 0.7\n",
      "(7 deepfake    ) acc: 79.2; ap: 93.9; r_acc: 1.0; f_acc: 0.6\n",
      "(8 Mean      ) acc: 89.2; ap: 95.3\n",
      "*************************\n",
      "2024_08_17_10_12_34\n",
      "2024_08_17_10_12_47 Train loss: 1.3820712183587602e-06 at step: 67600 lr 0.000162\n",
      "2024_08_17_10_13_52 Train loss: 2.3334154320764355e-05 at step: 68000 lr 0.000162\n",
      "2024_08_17_10_14_40 Train loss: 0.00041119707748293877 at step: 68400 lr 0.000162\n",
      "2024_08_17_10_15_42 Train loss: 0.0015627305256202817 at step: 68800 lr 0.000162\n",
      "2024_08_17_10_16_47 Train loss: 2.7939572078139463e-07 at step: 69200 lr 0.000162\n",
      "2024_08_17_10_17_58 Train loss: 9.468322787142824e-06 at step: 69600 lr 0.000162\n",
      "2024_08_17_10_19_04 Train loss: 0.007253177929669619 at step: 70000 lr 0.000162\n",
      "2024_08_17_10_20_11 Train loss: 1.9303446606500074e-05 at step: 70400 lr 0.000162\n",
      "2024_08_17_10_21_03 Train loss: 3.1340052373707294e-05 at step: 70800 lr 0.000162\n",
      "2024_08_17_10_21_54 Train loss: 4.157156126893824e-06 at step: 71200 lr 0.000162\n",
      "2024_08_17_10_22_57 Train loss: 4.842650014325045e-06 at step: 71600 lr 0.000162\n",
      "2024_08_17_10_23_37 Train loss: 1.1175869119028903e-08 at step: 72000 lr 0.000162\n",
      "2024_08_17_10_23_39 changing lr at the end of epoch 15, iters 72016\n",
      "*************************\n",
      "Changing lr from 0.000162 to 0.00014580000000000002\n",
      "*************************\n",
      "(Val @ epoch 15) acc: 0.985; ap: 0.9999287590258277\n",
      "Saving model ./checkpoints/local-grad-attention-cls-combine-2024_08_17_07_55_01/model_epoch_last.pth\n",
      "Saving model ./checkpoints/local-grad-attention-cls-combine-2024_08_17_07_55_01/model_epoch_15.pth\n",
      "early stop count 0/15\n",
      "*************************\n",
      "2024_08_17_10_23_41\n",
      "(0 progan      ) acc: 98.3; ap: 100.0; r_acc: 1.0; f_acc: 1.0\n",
      "(1 stylegan    ) acc: 96.9; ap: 100.0; r_acc: 1.0; f_acc: 0.9\n",
      "(2 stylegan2   ) acc: 93.4; ap: 99.9; r_acc: 1.0; f_acc: 0.9\n",
      "(3 biggan      ) acc: 87.2; ap: 94.9; r_acc: 0.8; f_acc: 0.9\n",
      "(4 cyclegan    ) acc: 81.8; ap: 95.2; r_acc: 1.0; f_acc: 0.7\n",
      "(5 stargan     ) acc: 100.0; ap: 100.0; r_acc: 1.0; f_acc: 1.0\n",
      "(6 gaugan      ) acc: 78.5; ap: 85.5; r_acc: 0.7; f_acc: 0.8\n",
      "(7 deepfake    ) acc: 60.8; ap: 92.7; r_acc: 1.0; f_acc: 0.2\n",
      "(8 Mean      ) acc: 87.1; ap: 96.0\n",
      "*************************\n",
      "2024_08_17_10_24_05\n",
      "2024_08_17_10_24_44 Train loss: 1.2889397567050764e-06 at step: 72400 lr 0.00014580000000000002\n"
     ]
    }
   ],
   "source": [
    "#experiment-01-no-filter\n",
    "!find /workspace/datasets -type d -name \"*ipynb*\" -exec rm -r {} +\n",
    "!python train.py \\\n",
    "--name local-grad-attention-cls-combine- \\\n",
    "--dataroot /workspace/datasets/ForenSynths_train_val \\\n",
    "--detect_method local_grad \\\n",
    "--num_thread 4 \\\n",
    "--classes car,cat,chair,horse --batch_size 32 --delr_freq 5 --lr 0.0002 --niter 30"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## Owner avatar\n",
    "FreqNet-DeepfakeDetection\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "!git clone https://github.com/chuangchuangtan/FreqNet-DeepfakeDetection.git"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "%cd /workspace/FreqNet-DeepfakeDetection"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "!python train.py \\\n",
    "--name 4class-resnet-car-cat-chair-horse \\\n",
    "--dataroot /workspace/datasets/ForenSynths_train_val_24000 \\\n",
    "--classes car,cat,chair,horse \\\n",
    "--batch_size 32 --delr_freq 10 --lr 0.001 --niter 85"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": []
  }
 ],
 "metadata": {
  "accelerator": "GPU",
  "colab": {
   "gpuType": "T4",
   "provenance": []
  },
  "kernelspec": {
   "display_name": "Python 3 (ipykernel)",
   "language": "python",
   "name": "python3"
  },
  "language_info": {
   "codemirror_mode": {
    "name": "ipython",
    "version": 3
   },
   "file_extension": ".py",
   "mimetype": "text/x-python",
   "name": "python",
   "nbconvert_exporter": "python",
   "pygments_lexer": "ipython3",
   "version": "3.10.13"
  }
 },
 "nbformat": 4,
 "nbformat_minor": 4
}
