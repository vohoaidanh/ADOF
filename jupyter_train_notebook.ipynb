{
  "cells": [
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "4bsPZQKgE3tX"
      },
      "source": [
        "## AIGCDetectBenchmark\n"
      ]
    },
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "ZiHwX8vIN8xm"
      },
      "source": [
        "## Install requirement"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": 1,
      "metadata": {
        "colab": {
          "base_uri": "https://localhost:8080/"
        },
        "id": "zedAZuw9N8xo",
        "outputId": "6a3b1b27-e105-4676-e438-901522f2b7cc"
      },
      "outputs": [
        {
          "output_type": "execute_result",
          "data": {
            "text/plain": [
              "CompletedProcess(args=['git', 'clone', 'https://github.com/vohoaidanh/ADOF.git'], returncode=0)"
            ]
          },
          "metadata": {},
          "execution_count": 1
        }
      ],
      "source": [
        "#!git clone https://github.com/vohoaidanh/ADOF.git\n",
        "\n",
        "import subprocess\n",
        "# Clone với HTTPS + Personal Access Token\n",
        "#token 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\n",
        "repo_url = \"https://github.com/vohoaidanh/ADOF.git\"\n",
        "subprocess.run([\"git\", \"clone\", repo_url])"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": 2,
      "metadata": {
        "id": "DA3oo09a5rS0"
      },
      "outputs": [],
      "source": [
        "USED_COLAB = True\n",
        "\n",
        "if USED_COLAB:\n",
        "    root_dir = \"/content\"\n",
        "else:\n",
        "    root_dir = \"workspace\""
      ]
    },
    {
      "cell_type": "code",
      "execution_count": 3,
      "metadata": {
        "id": "diL52UgAPQSZ",
        "outputId": "e231220b-1ce7-4da0-92f7-04146c9d3602",
        "colab": {
          "base_uri": "https://localhost:8080/"
        }
      },
      "outputs": [
        {
          "output_type": "stream",
          "name": "stdout",
          "text": [
            "/content/ADOF\n"
          ]
        }
      ],
      "source": [
        "%cd ADOF"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": 4,
      "metadata": {
        "id": "Clr6GZVH5rS0",
        "outputId": "50f679e8-9dbb-4c89-fbdc-4445f8fde346",
        "colab": {
          "base_uri": "https://localhost:8080/"
        }
      },
      "outputs": [
        {
          "output_type": "stream",
          "name": "stdout",
          "text": [
            "Branch 'exp/kd' set up to track remote branch 'exp/kd' from 'origin'.\n",
            "Switched to a new branch 'exp/kd'\n"
          ]
        }
      ],
      "source": [
        "!git checkout exp/kd"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": null,
      "metadata": {
        "scrolled": true,
        "id": "zyWIi-Is5rS0"
      },
      "outputs": [],
      "source": [
        "!pip install gdown -q"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": null,
      "metadata": {
        "id": "yZtRWDeMing8",
        "scrolled": true
      },
      "outputs": [],
      "source": [
        "#!pip install -r requirements.txt\n",
        "#!pip install tensorboardX -q\n"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": 5,
      "metadata": {
        "id": "zETSn3yrN8xq",
        "scrolled": true,
        "outputId": "9aef1f8f-2ff3-4862-ee0d-4f4a69befb79",
        "colab": {
          "base_uri": "https://localhost:8080/"
        }
      },
      "outputs": [
        {
          "output_type": "stream",
          "name": "stdout",
          "text": [
            "Reading package lists...\n",
            "Building dependency tree...\n",
            "Reading state information...\n",
            "unzip is already the newest version (6.0-26ubuntu3.2).\n",
            "0 upgraded, 0 newly installed, 0 to remove and 29 not upgraded.\n",
            "Reading package lists...\n",
            "Building dependency tree...\n",
            "Reading state information...\n",
            "zip is already the newest version (3.0-12build2).\n",
            "0 upgraded, 0 newly installed, 0 to remove and 29 not upgraded.\n",
            "\u001b[2K   \u001b[90m━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━\u001b[0m \u001b[32m101.7/101.7 kB\u001b[0m \u001b[31m3.4 MB/s\u001b[0m eta \u001b[36m0:00:00\u001b[0m\n",
            "\u001b[?25hReading package lists...\n",
            "Building dependency tree...\n",
            "Reading state information...\n",
            "The following NEW packages will be installed:\n",
            "  libgl1-mesa-glx\n",
            "0 upgraded, 1 newly installed, 0 to remove and 29 not upgraded.\n",
            "Need to get 5,584 B of archives.\n",
            "After this operation, 74.8 kB of additional disk space will be used.\n",
            "Get:1 http://archive.ubuntu.com/ubuntu jammy-updates/universe amd64 libgl1-mesa-glx amd64 23.0.4-0ubuntu1~22.04.1 [5,584 B]\n",
            "Fetched 5,584 B in 0s (38.7 kB/s)\n",
            "Selecting previously unselected package libgl1-mesa-glx:amd64.\n",
            "(Reading database ... 124947 files and directories currently installed.)\n",
            "Preparing to unpack .../libgl1-mesa-glx_23.0.4-0ubuntu1~22.04.1_amd64.deb ...\n",
            "Unpacking libgl1-mesa-glx:amd64 (23.0.4-0ubuntu1~22.04.1) ...\n",
            "Setting up libgl1-mesa-glx:amd64 (23.0.4-0ubuntu1~22.04.1) ...\n",
            "\u001b[2K   \u001b[90m━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━\u001b[0m \u001b[32m44.8/44.8 kB\u001b[0m \u001b[31m2.5 MB/s\u001b[0m eta \u001b[36m0:00:00\u001b[0m\n",
            "\u001b[2K   \u001b[90m━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━\u001b[0m \u001b[32m75.4/75.4 kB\u001b[0m \u001b[31m5.4 MB/s\u001b[0m eta \u001b[36m0:00:00\u001b[0m\n",
            "\u001b[2K   \u001b[90m━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━\u001b[0m \u001b[32m2.3/2.3 MB\u001b[0m \u001b[31m47.1 MB/s\u001b[0m eta \u001b[36m0:00:00\u001b[0m\n",
            "\u001b[2K   \u001b[90m━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━\u001b[0m \u001b[32m363.4/363.4 MB\u001b[0m \u001b[31m3.8 MB/s\u001b[0m eta \u001b[36m0:00:00\u001b[0m\n",
            "\u001b[2K   \u001b[90m━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━\u001b[0m \u001b[32m13.8/13.8 MB\u001b[0m \u001b[31m65.7 MB/s\u001b[0m eta \u001b[36m0:00:00\u001b[0m\n",
            "\u001b[2K   \u001b[90m━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━\u001b[0m \u001b[32m24.6/24.6 MB\u001b[0m \u001b[31m34.6 MB/s\u001b[0m eta \u001b[36m0:00:00\u001b[0m\n",
            "\u001b[2K   \u001b[90m━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━\u001b[0m \u001b[32m883.7/883.7 kB\u001b[0m \u001b[31m43.9 MB/s\u001b[0m eta \u001b[36m0:00:00\u001b[0m\n",
            "\u001b[2K   \u001b[90m━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━\u001b[0m \u001b[32m664.8/664.8 MB\u001b[0m \u001b[31m2.0 MB/s\u001b[0m eta \u001b[36m0:00:00\u001b[0m\n",
            "\u001b[2K   \u001b[90m━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━\u001b[0m \u001b[32m211.5/211.5 MB\u001b[0m \u001b[31m5.9 MB/s\u001b[0m eta \u001b[36m0:00:00\u001b[0m\n",
            "\u001b[2K   \u001b[90m━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━\u001b[0m \u001b[32m56.3/56.3 MB\u001b[0m \u001b[31m14.8 MB/s\u001b[0m eta \u001b[36m0:00:00\u001b[0m\n",
            "\u001b[2K   \u001b[90m━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━\u001b[0m \u001b[32m127.9/127.9 MB\u001b[0m \u001b[31m7.3 MB/s\u001b[0m eta \u001b[36m0:00:00\u001b[0m\n",
            "\u001b[2K   \u001b[90m━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━\u001b[0m \u001b[32m207.5/207.5 MB\u001b[0m \u001b[31m5.7 MB/s\u001b[0m eta \u001b[36m0:00:00\u001b[0m\n",
            "\u001b[2K   \u001b[90m━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━\u001b[0m \u001b[32m21.1/21.1 MB\u001b[0m \u001b[31m96.8 MB/s\u001b[0m eta \u001b[36m0:00:00\u001b[0m\n",
            "\u001b[2K   \u001b[90m━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━\u001b[0m \u001b[32m723.1/723.1 kB\u001b[0m \u001b[31m16.6 MB/s\u001b[0m eta \u001b[36m0:00:00\u001b[0m\n",
            "\u001b[2K   \u001b[90m━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━\u001b[0m \u001b[32m1.0/1.0 MB\u001b[0m \u001b[31m46.2 MB/s\u001b[0m eta \u001b[36m0:00:00\u001b[0m\n",
            "\u001b[2K   \u001b[90m━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━\u001b[0m \u001b[32m3.5/3.5 MB\u001b[0m \u001b[31m99.1 MB/s\u001b[0m eta \u001b[36m0:00:00\u001b[0m\n",
            "\u001b[2K   \u001b[90m━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━\u001b[0m \u001b[32m144.8/144.8 kB\u001b[0m \u001b[31m12.0 MB/s\u001b[0m eta \u001b[36m0:00:00\u001b[0m\n",
            "\u001b[?25h"
          ]
        }
      ],
      "source": [
        "# Without google colab\n",
        "!apt-get install -y unzip -q\n",
        "!apt-get install -y zip -q\n",
        "!pip install tensorboardX -q\n",
        "!pip install regex -q\n",
        "!pip install imageio -q\n",
        "!pip install opencv-python -q\n",
        "!apt-get install -y libgl1-mesa-glx -q\n",
        "!pip install scikit-learn -q\n",
        "!pip install scikit-image -q\n",
        "!pip install ftfy -q\n",
        "!pip install natsort -q\n",
        "!pip install blobfile -q\n",
        "!pip install timm -q\n",
        "!pip install comet_ml -q"
      ]
    },
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "4myLsJr75rS1"
      },
      "source": [
        "# Unzip Data from Gdrive"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": null,
      "metadata": {
        "id": "iH8CU8_W5rS1"
      },
      "outputs": [],
      "source": [
        "!mkdir -p $root_dir/datasets/ForenSynths_train\n",
        "destination = root_dir + \"/ForenSynths_train_val_24000.zip\"\n",
        "!unzip -q $destination -d $root_dir/datasets/ForenSynths_train"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": null,
      "metadata": {
        "id": "o-IKvxI_5rS1"
      },
      "outputs": [],
      "source": [
        "!mkdir -p $root_dir/datasets/ForenSynths_train/test\n",
        "destination = root_dir + \"/ForenSynths_test_200.zip\"\n",
        "!unzip -q $destination -d $root_dir/datasets/ForenSynths_train/test"
      ]
    },
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "5zY128DjN8xt"
      },
      "source": [
        "## Train"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": null,
      "metadata": {
        "colab": {
          "base_uri": "https://localhost:8080/"
        },
        "id": "0tTnMSRWN8xu",
        "outputId": "4d3c9462-ddc8-4810-dceb-2cb917dad37d",
        "scrolled": true
      },
      "outputs": [
        {
          "output_type": "stream",
          "name": "stdout",
          "text": [
            "find: ‘/datasets’: No such file or directory\n",
            "----------------- Options ---------------\n",
            "                     arch: res50                         \n",
            "                 backbone: adof_distill                  \t[default: resnet50]\n",
            "               batch_size: 64                            \n",
            "                    beta1: 0.9                           \n",
            "                blur_prob: 0                             \n",
            "                 blur_sig: 0.5                           \n",
            "          checkpoints_dir: ./checkpoints                 \n",
            "                class_bal: False                         \n",
            "                  classes: car,cat,chair,horse           \t[default: ]\n",
            "           continue_train: False                         \n",
            "                 cropSize: 224                           \n",
            "                 data_aug: False                         \n",
            "                 dataroot: /content/datasets/ForenSynths_train\t[default: ./dataset/]\n",
            "                delr_freq: 5                             \t[default: 20]\n",
            "          earlystop_epoch: 15                            \n",
            "                    epoch: latest                        \n",
            "              epoch_count: 1                             \n",
            "                  gpu_ids: 0                             \n",
            "                init_gain: 0.02                          \n",
            "                init_type: normal                        \n",
            "                  isTrain: True                          \t[default: None]\n",
            "               jpg_method: cv2                           \n",
            "                 jpg_prob: 0                             \n",
            "                 jpg_qual: 75                            \n",
            "               last_epoch: -1                            \n",
            "                 loadSize: 256                           \n",
            "                loss_freq: 50                            \t[default: 400]\n",
            "                       lr: 0.0002                        \t[default: 0.0001]\n",
            "                     mode: binary                        \n",
            "                     name: adof_distill-alpha_02025_03_10_07_18_34\t[default: experiment_name]\n",
            "                new_optim: False                         \n",
            "                    niter: 50                            \t[default: 1000]\n",
            "                  no_flip: False                         \n",
            "             num_features: auto                          \n",
            "              num_threads: 2                             \t[default: 8]\n",
            "           old_checkpoint: None                          \n",
            "                    optim: adam                          \n",
            "           resize_or_crop: scale_and_crop                \n",
            "                rz_interp: bilinear                      \n",
            "          save_epoch_freq: 20                            \n",
            "         save_latest_freq: 2000                          \n",
            "           serial_batches: False                         \n",
            "                   suffix:                               \n",
            "              train_split: train                         \n",
            "                use_comet: True                          \t[default: False]\n",
            "                val_split: val                           \n",
            "----------------- End -------------------\n",
            "\u001b[1;38;5;214mCOMET WARNING:\u001b[0m To get all data logged automatically, import comet_ml before the following modules: torch.\n",
            "\u001b[1;38;5;39mCOMET INFO:\u001b[0m Experiment is live on comet.com \u001b[38;5;39mhttps://www.comet.com/danhvohoai2-gmail-com/adof/bb4c46a719414fa8a35e24ce251f7e50\u001b[0m\n",
            "\n",
            "train.py  --name  adof_distill-alpha_0  --dataroot  /content/datasets/ForenSynths_train  --num_thread  2  --classes  car,cat,chair,horse  --batch_size  64  --delr_freq  5  --loss_freq  50  --lr  0.0002  --niter  50  --backbone  adof_distill  --gpu_ids  0  --use_comet\n",
            "/content/ADOF/networks/trainer.py:30: FutureWarning: You are using `torch.load` with `weights_only=False` (the current default value), which uses the default pickle module implicitly. It is possible to construct malicious pickle data which will execute arbitrary code during unpickling (See https://github.com/pytorch/pytorch/blob/main/SECURITY.md#untrusted-models for more details). In a future release, the default value for `weights_only` will be flipped to `True`. This limits the functions that could be executed during unpickling. Arbitrary objects will no longer be allowed to be loaded via this mode unless they are explicitly allowlisted by the user via `torch.serialization.add_safe_globals`. We recommend you start setting `weights_only=True` for any use case where you don't have full control of the loaded file. Please open an issue on GitHub for any issues related to this experimental feature.\n",
            "  self.model_teacher.load_state_dict(torch.load(\"./weights/ADOF_model_epoch_9.pth\", map_location='cpu'), strict=True)\n",
            "\n",
            "*************************\n",
            "2025_03_10_07_18_36\n",
            "(0 progan      ) acc: 50.0000; ap: 43.4459; r_acc: 0.0000; f_acc: 1.0000\n",
            "(1 stylegan    ) acc: 50.0000; ap: 36.4010; r_acc: 0.0000; f_acc: 1.0000\n",
            "(2 stylegan2   ) acc: 50.0000; ap: 36.6744; r_acc: 0.0000; f_acc: 1.0000\n",
            "(3 biggan      ) acc: 50.0000; ap: 45.6442; r_acc: 0.0000; f_acc: 1.0000\n",
            "(4 cyclegan    ) acc: 50.0000; ap: 37.7292; r_acc: 0.0000; f_acc: 1.0000\n",
            "(5 stargan     ) acc: 50.0000; ap: 31.5723; r_acc: 0.0000; f_acc: 1.0000\n",
            "(6 gaugan      ) acc: 50.0000; ap: 45.6012; r_acc: 0.0000; f_acc: 1.0000\n",
            "(7 deepfake    ) acc: 50.0000; ap: 58.7035; r_acc: 0.0000; f_acc: 1.0000\n",
            "(8 Mean      ) acc: 50.0000; ap: 41.9714\n",
            "*************************\n",
            "2025_03_10_07_19_39\n",
            "cwd: /content/ADOF\n",
            "2025_03_10_07_20_00 Train loss/d_loss: 0.5970942974090576/1.938401699066162 at step: 50 lr 0.0002\n",
            "2025_03_10_07_20_20 Train loss/d_loss: 0.49265551567077637/1.8335602283477783 at step: 100 lr 0.0002\n",
            "2025_03_10_07_20_41 Train loss/d_loss: 0.3007727861404419/1.8442656993865967 at step: 150 lr 0.0002\n",
            "2025_03_10_07_21_00 Train loss/d_loss: 0.4204767346382141/1.7567262649536133 at step: 200 lr 0.0002\n",
            "2025_03_10_07_21_21 Train loss/d_loss: 0.23290696740150452/1.7431683540344238 at step: 250 lr 0.0002\n",
            "2025_03_10_07_21_43 Train loss/d_loss: 0.21796837449073792/1.8183708190917969 at step: 300 lr 0.0002\n",
            "2025_03_10_07_22_02 Train loss/d_loss: 0.10076200217008591/1.8240607976913452 at step: 350 lr 0.0002\n",
            "Saving model ./checkpoints/adof_distill-alpha_02025_03_10_07_18_34/model_epoch_0.pth\n",
            " Epoch 0 : 2025_03_10_07_19_39 --> 2025_03_10_07_22_12\n",
            "Saving model ./checkpoints/adof_distill-alpha_02025_03_10_07_18_34/model_epoch_last.pth\n",
            "(Val @ epoch 0) acc: 0.795625; ap: 0.9599491707810205\n",
            "*************************\n",
            "2025_03_10_07_22_20\n",
            "(0 progan      ) acc: 79.3625; ap: 94.7535; r_acc: 0.9830; f_acc: 0.6042\n",
            "(1 stylegan    ) acc: 80.2500; ap: 96.7543; r_acc: 0.9900; f_acc: 0.6150\n",
            "(2 stylegan2   ) acc: 87.8750; ap: 97.7718; r_acc: 0.9888; f_acc: 0.7688\n",
            "(3 biggan      ) acc: 63.2500; ap: 73.1988; r_acc: 0.8300; f_acc: 0.4350\n",
            "(4 cyclegan    ) acc: 52.1226; ap: 62.6190; r_acc: 0.8981; f_acc: 0.1443\n",
            "(5 stargan     ) acc: 58.5000; ap: 96.7643; r_acc: 1.0000; f_acc: 0.1700\n",
            "(6 gaugan      ) acc: 42.0000; ap: 46.4395; r_acc: 0.7200; f_acc: 0.1200\n",
            "(7 deepfake    ) acc: 52.0000; ap: 89.7651; r_acc: 1.0000; f_acc: 0.0400\n",
            "(8 Mean      ) acc: 64.4200; ap: 82.2583\n",
            "*************************\n",
            "2025_03_10_07_23_24\n",
            "2025_03_10_07_23_34 Train loss/d_loss: 0.1460256427526474/1.7190091609954834 at step: 400 lr 0.0002\n",
            "2025_03_10_07_23_54 Train loss/d_loss: 0.10689417272806168/1.709238052368164 at step: 450 lr 0.0002\n",
            "2025_03_10_07_24_14 Train loss/d_loss: 0.20413699746131897/1.7427932024002075 at step: 500 lr 0.0002\n",
            "2025_03_10_07_24_33 Train loss/d_loss: 0.09371691942214966/1.7322402000427246 at step: 550 lr 0.0002\n",
            "2025_03_10_07_24_53 Train loss/d_loss: 0.1278209686279297/1.7240962982177734 at step: 600 lr 0.0002\n",
            "2025_03_10_07_25_12 Train loss/d_loss: 0.11830595135688782/1.679447889328003 at step: 650 lr 0.0002\n",
            "2025_03_10_07_25_32 Train loss/d_loss: 0.07517406344413757/1.6120433807373047 at step: 700 lr 0.0002\n",
            "2025_03_10_07_25_52 Train loss/d_loss: 0.11315002292394638/1.7124316692352295 at step: 750 lr 0.0002\n",
            "Saving model ./checkpoints/adof_distill-alpha_02025_03_10_07_18_34/model_epoch_1.pth\n",
            " Epoch 1 : 2025_03_10_07_23_24 --> 2025_03_10_07_25_52\n",
            "Saving model ./checkpoints/adof_distill-alpha_02025_03_10_07_18_34/model_epoch_last.pth\n",
            "(Val @ epoch 1) acc: 0.5025; ap: 0.9467346152383308\n",
            "*************************\n",
            "2025_03_10_07_25_59\n",
            "(0 progan      ) acc: 51.4250; ap: 93.4170; r_acc: 1.0000; f_acc: 0.0285\n",
            "(1 stylegan    ) acc: 50.2500; ap: 95.9349; r_acc: 1.0000; f_acc: 0.0050\n",
            "(2 stylegan2   ) acc: 50.8750; ap: 96.2564; r_acc: 1.0000; f_acc: 0.0175\n",
            "(3 biggan      ) acc: 51.7500; ap: 72.8572; r_acc: 1.0000; f_acc: 0.0350\n",
            "(4 cyclegan    ) acc: 50.5189; ap: 82.4464; r_acc: 1.0000; f_acc: 0.0104\n",
            "(5 stargan     ) acc: 50.2500; ap: 97.7775; r_acc: 1.0000; f_acc: 0.0050\n",
            "(6 gaugan      ) acc: 50.5000; ap: 71.9048; r_acc: 1.0000; f_acc: 0.0100\n",
            "(7 deepfake    ) acc: 50.0000; ap: 82.0439; r_acc: 1.0000; f_acc: 0.0000\n",
            "(8 Mean      ) acc: 50.6961; ap: 86.5798\n",
            "*************************\n",
            "2025_03_10_07_27_01\n",
            "2025_03_10_07_27_21 Train loss/d_loss: 0.07954704761505127/1.6419767141342163 at step: 800 lr 0.0002\n",
            "2025_03_10_07_27_41 Train loss/d_loss: 0.08352634310722351/1.6597821712493896 at step: 850 lr 0.0002\n",
            "2025_03_10_07_28_00 Train loss/d_loss: 0.12637604773044586/1.7347608804702759 at step: 900 lr 0.0002\n",
            "2025_03_10_07_28_20 Train loss/d_loss: 0.06862734258174896/1.686732530593872 at step: 950 lr 0.0002\n",
            "2025_03_10_07_28_39 Train loss/d_loss: 0.06585366278886795/1.6646485328674316 at step: 1000 lr 0.0002\n",
            "2025_03_10_07_28_59 Train loss/d_loss: 0.08045235276222229/1.72931706905365 at step: 1050 lr 0.0002\n",
            "2025_03_10_07_29_19 Train loss/d_loss: 0.06974834203720093/1.6730715036392212 at step: 1100 lr 0.0002\n",
            "Saving model ./checkpoints/adof_distill-alpha_02025_03_10_07_18_34/model_epoch_2.pth\n",
            " Epoch 2 : 2025_03_10_07_27_01 --> 2025_03_10_07_29_29\n",
            "Saving model ./checkpoints/adof_distill-alpha_02025_03_10_07_18_34/model_epoch_last.pth\n",
            "(Val @ epoch 2) acc: 0.7725; ap: 0.9958611658799289\n",
            "*************************\n",
            "2025_03_10_07_29_35\n",
            "(0 progan      ) acc: 76.2125; ap: 99.3030; r_acc: 0.9998; f_acc: 0.5245\n",
            "(1 stylegan    ) acc: 69.8333; ap: 99.0123; r_acc: 1.0000; f_acc: 0.3967\n",
            "(2 stylegan2   ) acc: 74.7500; ap: 99.1468; r_acc: 1.0000; f_acc: 0.4950\n",
            "(3 biggan      ) acc: 60.5000; ap: 85.5062; r_acc: 0.9950; f_acc: 0.2150\n",
            "(4 cyclegan    ) acc: 52.4057; ap: 84.3647; r_acc: 0.9981; f_acc: 0.0500\n",
            "(5 stargan     ) acc: 56.7500; ap: 98.9800; r_acc: 1.0000; f_acc: 0.1350\n",
            "(6 gaugan      ) acc: 54.5000; ap: 78.7462; r_acc: 0.9750; f_acc: 0.1150\n",
            "(7 deepfake    ) acc: 54.7500; ap: 89.1406; r_acc: 0.9950; f_acc: 0.1000\n",
            "(8 Mean      ) acc: 62.4627; ap: 91.7750\n",
            "*************************\n",
            "2025_03_10_07_30_37\n",
            "2025_03_10_07_30_48 Train loss/d_loss: 0.0760064423084259/1.660652995109558 at step: 1150 lr 0.0002\n",
            "2025_03_10_07_31_08 Train loss/d_loss: 0.07259155064821243/1.634755253791809 at step: 1200 lr 0.0002\n",
            "2025_03_10_07_31_27 Train loss/d_loss: 0.0384945273399353/1.587336778640747 at step: 1250 lr 0.0002\n",
            "2025_03_10_07_31_47 Train loss/d_loss: 0.025132592767477036/1.7286040782928467 at step: 1300 lr 0.0002\n",
            "2025_03_10_07_32_07 Train loss/d_loss: 0.10361307859420776/1.733127474784851 at step: 1350 lr 0.0002\n",
            "2025_03_10_07_32_26 Train loss/d_loss: 0.03315361216664314/1.7209898233413696 at step: 1400 lr 0.0002\n",
            "2025_03_10_07_32_46 Train loss/d_loss: 0.021012157201766968/1.7507797479629517 at step: 1450 lr 0.0002\n",
            "2025_03_10_07_33_05 Train loss/d_loss: 0.03301762789487839/1.675214409828186 at step: 1500 lr 0.0002\n",
            "Saving model ./checkpoints/adof_distill-alpha_02025_03_10_07_18_34/model_epoch_3.pth\n",
            " Epoch 3 : 2025_03_10_07_30_37 --> 2025_03_10_07_33_06\n",
            "Saving model ./checkpoints/adof_distill-alpha_02025_03_10_07_18_34/model_epoch_last.pth\n",
            "(Val @ epoch 3) acc: 0.5; ap: 0.8513594577628951\n",
            "*************************\n",
            "2025_03_10_07_33_12\n",
            "(0 progan      ) acc: 50.2500; ap: 85.7643; r_acc: 1.0000; f_acc: 0.0050\n",
            "(1 stylegan    ) acc: 50.0000; ap: 87.3434; r_acc: 1.0000; f_acc: 0.0000\n",
            "(2 stylegan2   ) acc: 50.0000; ap: 81.1701; r_acc: 1.0000; f_acc: 0.0000\n",
            "(3 biggan      ) acc: 50.0000; ap: 64.3973; r_acc: 1.0000; f_acc: 0.0000\n",
            "(4 cyclegan    ) acc: 50.2358; ap: 85.0635; r_acc: 1.0000; f_acc: 0.0047\n",
            "(5 stargan     ) acc: 50.0000; ap: 96.2710; r_acc: 1.0000; f_acc: 0.0000\n",
            "(6 gaugan      ) acc: 50.0000; ap: 79.8864; r_acc: 1.0000; f_acc: 0.0000\n",
            "(7 deepfake    ) acc: 50.0000; ap: 74.1601; r_acc: 1.0000; f_acc: 0.0000\n",
            "(8 Mean      ) acc: 50.0607; ap: 81.7570\n",
            "*************************\n",
            "2025_03_10_07_34_14\n",
            "2025_03_10_07_34_34 Train loss/d_loss: 0.03582993894815445/1.6228594779968262 at step: 1550 lr 0.0002\n",
            "2025_03_10_07_34_54 Train loss/d_loss: 0.057698242366313934/1.6564655303955078 at step: 1600 lr 0.0002\n",
            "2025_03_10_07_35_13 Train loss/d_loss: 0.017268646508455276/1.6877782344818115 at step: 1650 lr 0.0002\n",
            "2025_03_10_07_35_33 Train loss/d_loss: 0.07483106851577759/1.6579456329345703 at step: 1700 lr 0.0002\n",
            "2025_03_10_07_35_53 Train loss/d_loss: 0.01478064339607954/1.7060233354568481 at step: 1750 lr 0.0002\n",
            "2025_03_10_07_36_12 Train loss/d_loss: 0.02276008576154709/1.706115961074829 at step: 1800 lr 0.0002\n",
            "2025_03_10_07_36_32 Train loss/d_loss: 0.0057101380079984665/1.721569299697876 at step: 1850 lr 0.0002\n",
            "Saving model ./checkpoints/adof_distill-alpha_02025_03_10_07_18_34/model_epoch_4.pth\n",
            " Epoch 4 : 2025_03_10_07_34_14 --> 2025_03_10_07_36_42\n",
            "Saving model ./checkpoints/adof_distill-alpha_02025_03_10_07_18_34/model_epoch_last.pth\n",
            "(Val @ epoch 4) acc: 0.556875; ap: 0.9940802993413755\n",
            "*************************\n",
            "2025_03_10_07_36_49\n",
            "(0 progan      ) acc: 57.4000; ap: 98.6643; r_acc: 0.1480; f_acc: 1.0000\n",
            "(1 stylegan    ) acc: 60.8333; ap: 96.9686; r_acc: 0.2167; f_acc: 1.0000\n",
            "(2 stylegan2   ) acc: 55.8750; ap: 98.7212; r_acc: 0.1175; f_acc: 1.0000\n",
            "(3 biggan      ) acc: 55.5000; ap: 78.1787; r_acc: 0.1100; f_acc: 1.0000\n",
            "(4 cyclegan    ) acc: 60.8491; ap: 60.8531; r_acc: 0.2179; f_acc: 0.9991\n",
            "(5 stargan     ) acc: 51.0000; ap: 89.1148; r_acc: 0.0200; f_acc: 1.0000\n",
            "(6 gaugan      ) acc: 52.5000; ap: 56.2638; r_acc: 0.0500; f_acc: 1.0000\n",
            "(7 deepfake    ) acc: 50.7500; ap: 77.8901; r_acc: 0.0150; f_acc: 1.0000\n",
            "(8 Mean      ) acc: 55.5884; ap: 82.0818\n",
            "*************************\n",
            "2025_03_10_07_37_53\n",
            "2025_03_10_07_38_03 Train loss/d_loss: 0.05953744798898697/1.71931791305542 at step: 1900 lr 0.0002\n",
            "2025_03_10_07_38_23 Train loss/d_loss: 0.044953323900699615/1.7375558614730835 at step: 1950 lr 0.0002\n",
            "2025_03_10_07_38_42 Train loss/d_loss: 0.07838881015777588/1.6664843559265137 at step: 2000 lr 0.0002\n",
            "2025_03_10_07_39_02 Train loss/d_loss: 0.0658923089504242/1.714759111404419 at step: 2050 lr 0.0002\n",
            "2025_03_10_07_39_22 Train loss/d_loss: 0.04067477211356163/1.6593894958496094 at step: 2100 lr 0.0002\n",
            "2025_03_10_07_39_41 Train loss/d_loss: 0.01805664598941803/1.6623470783233643 at step: 2150 lr 0.0002\n",
            "2025_03_10_07_40_01 Train loss/d_loss: 0.012590017169713974/1.7393062114715576 at step: 2200 lr 0.0002\n",
            "2025_03_10_07_40_20 Train loss/d_loss: 0.025021230801939964/1.7288274765014648 at step: 2250 lr 0.0002\n",
            "2025_03_10_07_40_20 changing lr at the end of epoch 5, iters 2250\n",
            "*************************\n",
            "Changing lr from 0.0002 to 0.00018\n",
            "*************************\n",
            "Saving model ./checkpoints/adof_distill-alpha_02025_03_10_07_18_34/model_epoch_5.pth\n",
            " Epoch 5 : 2025_03_10_07_37_53 --> 2025_03_10_07_40_20\n",
            "Saving model ./checkpoints/adof_distill-alpha_02025_03_10_07_18_34/model_epoch_last.pth\n",
            "(Val @ epoch 5) acc: 0.756875; ap: 0.9983443960170728\n",
            "*************************\n",
            "2025_03_10_07_40_27\n",
            "(0 progan      ) acc: 75.7625; ap: 99.6518; r_acc: 1.0000; f_acc: 0.5152\n",
            "(1 stylegan    ) acc: 72.4167; ap: 99.1725; r_acc: 1.0000; f_acc: 0.4483\n",
            "(2 stylegan2   ) acc: 75.8125; ap: 98.8462; r_acc: 1.0000; f_acc: 0.5162\n",
            "(3 biggan      ) acc: 56.5000; ap: 84.4761; r_acc: 1.0000; f_acc: 0.1300\n",
            "(4 cyclegan    ) acc: 52.0283; ap: 89.0917; r_acc: 1.0000; f_acc: 0.0406\n",
            "(5 stargan     ) acc: 58.0000; ap: 94.0058; r_acc: 1.0000; f_acc: 0.1600\n",
            "(6 gaugan      ) acc: 51.7500; ap: 83.4733; r_acc: 1.0000; f_acc: 0.0350\n",
            "(7 deepfake    ) acc: 61.5000; ap: 86.7554; r_acc: 0.9700; f_acc: 0.2600\n",
            "(8 Mean      ) acc: 62.9712; ap: 91.9341\n",
            "*************************\n",
            "2025_03_10_07_41_31\n",
            "2025_03_10_07_41_51 Train loss/d_loss: 0.04504161328077316/1.6654622554779053 at step: 2300 lr 0.00018\n",
            "2025_03_10_07_42_11 Train loss/d_loss: 0.008727227337658405/1.6561918258666992 at step: 2350 lr 0.00018\n",
            "2025_03_10_07_42_30 Train loss/d_loss: 0.01786182075738907/1.6833398342132568 at step: 2400 lr 0.00018\n",
            "2025_03_10_07_42_50 Train loss/d_loss: 0.025836732238531113/1.7638835906982422 at step: 2450 lr 0.00018\n",
            "2025_03_10_07_43_09 Train loss/d_loss: 0.0396529957652092/1.754465103149414 at step: 2500 lr 0.00018\n",
            "2025_03_10_07_43_29 Train loss/d_loss: 0.007518703117966652/1.7616444826126099 at step: 2550 lr 0.00018\n",
            "2025_03_10_07_43_49 Train loss/d_loss: 0.013801416382193565/1.7580631971359253 at step: 2600 lr 0.00018\n",
            "Saving model ./checkpoints/adof_distill-alpha_02025_03_10_07_18_34/model_epoch_6.pth\n",
            " Epoch 6 : 2025_03_10_07_41_31 --> 2025_03_10_07_43_59\n",
            "Saving model ./checkpoints/adof_distill-alpha_02025_03_10_07_18_34/model_epoch_last.pth\n",
            "(Val @ epoch 6) acc: 0.980625; ap: 0.9996298321428954\n",
            "*************************\n",
            "2025_03_10_07_44_06\n",
            "(0 progan      ) acc: 95.6125; ap: 99.7837; r_acc: 0.9970; f_acc: 0.9153\n",
            "(1 stylegan    ) acc: 89.8333; ap: 99.4603; r_acc: 1.0000; f_acc: 0.7967\n",
            "(2 stylegan2   ) acc: 93.5000; ap: 99.4899; r_acc: 1.0000; f_acc: 0.8700\n",
            "(3 biggan      ) acc: 82.7500; ap: 91.9634; r_acc: 0.8650; f_acc: 0.7900\n",
            "(4 cyclegan    ) acc: 71.4623; ap: 88.1968; r_acc: 0.9330; f_acc: 0.4962\n",
            "(5 stargan     ) acc: 64.5000; ap: 97.2711; r_acc: 0.9950; f_acc: 0.2950\n",
            "(6 gaugan      ) acc: 70.2500; ap: 75.2087; r_acc: 0.8050; f_acc: 0.6000\n",
            "(7 deepfake    ) acc: 57.2500; ap: 83.0221; r_acc: 0.9850; f_acc: 0.1600\n",
            "(8 Mean      ) acc: 78.1448; ap: 91.7995\n",
            "*************************\n",
            "2025_03_10_07_45_07\n",
            "2025_03_10_07_45_17 Train loss/d_loss: 0.009194604121148586/1.7209593057632446 at step: 2650 lr 0.00018\n",
            "2025_03_10_07_45_37 Train loss/d_loss: 0.023473002016544342/1.7015420198440552 at step: 2700 lr 0.00018\n",
            "2025_03_10_07_45_57 Train loss/d_loss: 0.020100001245737076/1.6699943542480469 at step: 2750 lr 0.00018\n",
            "2025_03_10_07_46_16 Train loss/d_loss: 0.010266334749758244/1.7211010456085205 at step: 2800 lr 0.00018\n",
            "2025_03_10_07_46_36 Train loss/d_loss: 0.062115173786878586/1.6923116445541382 at step: 2850 lr 0.00018\n",
            "2025_03_10_07_46_56 Train loss/d_loss: 0.04671613872051239/1.6810131072998047 at step: 2900 lr 0.00018\n",
            "2025_03_10_07_47_15 Train loss/d_loss: 0.021222228184342384/1.7489303350448608 at step: 2950 lr 0.00018\n",
            "2025_03_10_07_47_35 Train loss/d_loss: 0.001744079403579235/1.6684982776641846 at step: 3000 lr 0.00018\n",
            "Saving model ./checkpoints/adof_distill-alpha_02025_03_10_07_18_34/model_epoch_7.pth\n",
            " Epoch 7 : 2025_03_10_07_45_07 --> 2025_03_10_07_47_35\n",
            "Saving model ./checkpoints/adof_distill-alpha_02025_03_10_07_18_34/model_epoch_last.pth\n",
            "(Val @ epoch 7) acc: 0.97375; ap: 0.9971824467400046\n",
            "*************************\n",
            "2025_03_10_07_47_42\n",
            "(0 progan      ) acc: 95.7750; ap: 99.1450; r_acc: 0.9788; f_acc: 0.9367\n",
            "(1 stylegan    ) acc: 95.0000; ap: 99.1731; r_acc: 0.9883; f_acc: 0.9117\n",
            "(2 stylegan2   ) acc: 92.3125; ap: 97.5969; r_acc: 0.9838; f_acc: 0.8625\n",
            "(3 biggan      ) acc: 75.0000; ap: 90.9163; r_acc: 0.5350; f_acc: 0.9650\n",
            "(4 cyclegan    ) acc: 83.2547; ap: 84.0749; r_acc: 0.7236; f_acc: 0.9415\n",
            "(5 stargan     ) acc: 95.7500; ap: 99.5251; r_acc: 0.9950; f_acc: 0.9200\n",
            "(6 gaugan      ) acc: 70.7500; ap: 71.0992; r_acc: 0.4400; f_acc: 0.9750\n",
            "(7 deepfake    ) acc: 71.2500; ap: 86.2725; r_acc: 0.9400; f_acc: 0.4850\n",
            "(8 Mean      ) acc: 84.8865; ap: 90.9754\n",
            "*************************\n",
            "2025_03_10_07_48_43\n",
            "2025_03_10_07_49_03 Train loss/d_loss: 0.00497075729072094/1.701852560043335 at step: 3050 lr 0.00018\n",
            "2025_03_10_07_49_23 Train loss/d_loss: 0.022686252370476723/1.7282681465148926 at step: 3100 lr 0.00018\n",
            "2025_03_10_07_49_42 Train loss/d_loss: 0.006499615032225847/1.716040015220642 at step: 3150 lr 0.00018\n",
            "2025_03_10_07_50_02 Train loss/d_loss: 0.02660808525979519/1.675702452659607 at step: 3200 lr 0.00018\n",
            "2025_03_10_07_50_22 Train loss/d_loss: 0.0076166861690580845/1.7335453033447266 at step: 3250 lr 0.00018\n",
            "2025_03_10_07_50_41 Train loss/d_loss: 0.011472046375274658/1.7373239994049072 at step: 3300 lr 0.00018\n",
            "2025_03_10_07_51_01 Train loss/d_loss: 0.0029972908087074757/1.735698938369751 at step: 3350 lr 0.00018\n",
            "Saving model ./checkpoints/adof_distill-alpha_02025_03_10_07_18_34/model_epoch_8.pth\n",
            " Epoch 8 : 2025_03_10_07_48_43 --> 2025_03_10_07_51_11\n",
            "Saving model ./checkpoints/adof_distill-alpha_02025_03_10_07_18_34/model_epoch_last.pth\n",
            "(Val @ epoch 8) acc: 0.99; ap: 0.9998632951326494\n",
            "*************************\n",
            "2025_03_10_07_51_18\n",
            "(0 progan      ) acc: 98.8250; ap: 99.9149; r_acc: 0.9855; f_acc: 0.9910\n",
            "(1 stylegan    ) acc: 99.1667; ap: 99.9574; r_acc: 0.9933; f_acc: 0.9900\n",
            "(2 stylegan2   ) acc: 97.5625; ap: 99.6017; r_acc: 0.9800; f_acc: 0.9712\n",
            "(3 biggan      ) acc: 80.0000; ap: 89.9791; r_acc: 0.6400; f_acc: 0.9600\n",
            "(4 cyclegan    ) acc: 79.9057; ap: 87.1771; r_acc: 0.8264; f_acc: 0.7717\n",
            "(5 stargan     ) acc: 96.5000; ap: 99.4577; r_acc: 0.9400; f_acc: 0.9900\n",
            "(6 gaugan      ) acc: 76.0000; ap: 75.6607; r_acc: 0.5600; f_acc: 0.9600\n",
            "(7 deepfake    ) acc: 84.7500; ap: 91.7179; r_acc: 0.7450; f_acc: 0.9500\n",
            "(8 Mean      ) acc: 89.0887; ap: 92.9333\n",
            "*************************\n",
            "2025_03_10_07_52_19\n",
            "2025_03_10_07_52_29 Train loss/d_loss: 0.00895022228360176/1.6381449699401855 at step: 3400 lr 0.00018\n",
            "2025_03_10_07_52_49 Train loss/d_loss: 0.06959028542041779/1.645160436630249 at step: 3450 lr 0.00018\n",
            "2025_03_10_07_53_09 Train loss/d_loss: 0.034806858748197556/1.6021506786346436 at step: 3500 lr 0.00018\n",
            "2025_03_10_07_53_28 Train loss/d_loss: 0.014452187344431877/1.6531822681427002 at step: 3550 lr 0.00018\n",
            "2025_03_10_07_53_48 Train loss/d_loss: 0.01701272651553154/1.7312343120574951 at step: 3600 lr 0.00018\n",
            "2025_03_10_07_54_07 Train loss/d_loss: 0.053974516689777374/1.6841603517532349 at step: 3650 lr 0.00018\n",
            "2025_03_10_07_54_27 Train loss/d_loss: 0.03514532744884491/1.7084717750549316 at step: 3700 lr 0.00018\n",
            "2025_03_10_07_54_46 Train loss/d_loss: 0.004201270639896393/1.722680926322937 at step: 3750 lr 0.00018\n",
            "Saving model ./checkpoints/adof_distill-alpha_02025_03_10_07_18_34/model_epoch_9.pth\n",
            " Epoch 9 : 2025_03_10_07_52_19 --> 2025_03_10_07_54_47\n",
            "Saving model ./checkpoints/adof_distill-alpha_02025_03_10_07_18_34/model_epoch_last.pth\n",
            "(Val @ epoch 9) acc: 0.640625; ap: 0.9929568921303582\n",
            "*************************\n",
            "2025_03_10_07_54_53\n",
            "(0 progan      ) acc: 64.5375; ap: 98.4471; r_acc: 0.9998; f_acc: 0.2910\n",
            "(1 stylegan    ) acc: 65.1667; ap: 99.0137; r_acc: 1.0000; f_acc: 0.3033\n",
            "(2 stylegan2   ) acc: 69.1875; ap: 98.3340; r_acc: 1.0000; f_acc: 0.3837\n",
            "(3 biggan      ) acc: 60.5000; ap: 84.7532; r_acc: 0.9950; f_acc: 0.2150\n",
            "(4 cyclegan    ) acc: 55.0000; ap: 90.1749; r_acc: 0.9972; f_acc: 0.1028\n",
            "(5 stargan     ) acc: 51.5000; ap: 98.9711; r_acc: 1.0000; f_acc: 0.0300\n",
            "(6 gaugan      ) acc: 55.7500; ap: 76.6381; r_acc: 0.9650; f_acc: 0.1500\n",
            "(7 deepfake    ) acc: 50.7500; ap: 80.2770; r_acc: 1.0000; f_acc: 0.0150\n",
            "(8 Mean      ) acc: 59.0490; ap: 90.8261\n",
            "*************************\n",
            "2025_03_10_07_55_54\n",
            "2025_03_10_07_56_15 Train loss/d_loss: 0.022128012031316757/1.6871943473815918 at step: 3800 lr 0.00018\n",
            "2025_03_10_07_56_34 Train loss/d_loss: 0.031066346913576126/1.6807887554168701 at step: 3850 lr 0.00018\n",
            "2025_03_10_07_56_54 Train loss/d_loss: 0.0032706051133573055/1.6999800205230713 at step: 3900 lr 0.00018\n",
            "2025_03_10_07_57_13 Train loss/d_loss: 0.007063142955303192/1.705406904220581 at step: 3950 lr 0.00018\n",
            "2025_03_10_07_57_33 Train loss/d_loss: 0.008601159788668156/1.6652822494506836 at step: 4000 lr 0.00018\n",
            "2025_03_10_07_57_52 Train loss/d_loss: 0.01460100058466196/1.7286745309829712 at step: 4050 lr 0.00018\n",
            "2025_03_10_07_58_12 Train loss/d_loss: 0.02252812869846821/1.6555277109146118 at step: 4100 lr 0.00018\n",
            "2025_03_10_07_58_22 changing lr at the end of epoch 10, iters 4125\n",
            "*************************\n",
            "Changing lr from 0.00018 to 0.000162\n",
            "*************************\n",
            "Saving model ./checkpoints/adof_distill-alpha_02025_03_10_07_18_34/model_epoch_10.pth\n",
            " Epoch 10 : 2025_03_10_07_55_54 --> 2025_03_10_07_58_22\n",
            "Saving model ./checkpoints/adof_distill-alpha_02025_03_10_07_18_34/model_epoch_last.pth\n",
            "(Val @ epoch 10) acc: 0.50875; ap: 0.9160306584090618\n",
            "*************************\n",
            "2025_03_10_07_58_28\n",
            "(0 progan      ) acc: 51.3625; ap: 88.2901; r_acc: 1.0000; f_acc: 0.0272\n",
            "(1 stylegan    ) acc: 52.0833; ap: 90.2966; r_acc: 1.0000; f_acc: 0.0417\n",
            "(2 stylegan2   ) acc: 54.5000; ap: 92.5856; r_acc: 1.0000; f_acc: 0.0900\n",
            "(3 biggan      ) acc: 50.5000; ap: 69.9542; r_acc: 1.0000; f_acc: 0.0100\n",
            "(4 cyclegan    ) acc: 50.0472; ap: 85.7583; r_acc: 1.0000; f_acc: 0.0009\n",
            "(5 stargan     ) acc: 50.0000; ap: 80.9210; r_acc: 1.0000; f_acc: 0.0000\n",
            "(6 gaugan      ) acc: 50.2500; ap: 73.1780; r_acc: 1.0000; f_acc: 0.0050\n",
            "(7 deepfake    ) acc: 50.0000; ap: 59.1141; r_acc: 1.0000; f_acc: 0.0000\n",
            "(8 Mean      ) acc: 51.0929; ap: 80.0122\n",
            "*************************\n",
            "2025_03_10_07_59_29\n",
            "2025_03_10_07_59_40 Train loss/d_loss: 0.015305922366678715/1.6955058574676514 at step: 4150 lr 0.000162\n",
            "2025_03_10_08_00_00 Train loss/d_loss: 0.007183126173913479/1.6934489011764526 at step: 4200 lr 0.000162\n",
            "2025_03_10_08_00_19 Train loss/d_loss: 0.006543845869600773/1.6929634809494019 at step: 4250 lr 0.000162\n",
            "2025_03_10_08_00_39 Train loss/d_loss: 0.006139760836958885/1.6917290687561035 at step: 4300 lr 0.000162\n",
            "2025_03_10_08_00_58 Train loss/d_loss: 0.005360464099794626/1.6848347187042236 at step: 4350 lr 0.000162\n",
            "2025_03_10_08_01_18 Train loss/d_loss: 0.0027463864535093307/1.680896282196045 at step: 4400 lr 0.000162\n",
            "2025_03_10_08_01_38 Train loss/d_loss: 0.13397270441055298/1.680436372756958 at step: 4450 lr 0.000162\n",
            "2025_03_10_08_01_57 Train loss/d_loss: 0.005422509741038084/1.6712110042572021 at step: 4500 lr 0.000162\n",
            "Saving model ./checkpoints/adof_distill-alpha_02025_03_10_07_18_34/model_epoch_11.pth\n",
            " Epoch 11 : 2025_03_10_07_59_29 --> 2025_03_10_08_01_57\n",
            "Saving model ./checkpoints/adof_distill-alpha_02025_03_10_07_18_34/model_epoch_last.pth\n",
            "(Val @ epoch 11) acc: 0.974375; ap: 0.998025481007555\n",
            "*************************\n",
            "2025_03_10_08_02_03\n",
            "(0 progan      ) acc: 95.6500; ap: 99.3686; r_acc: 0.9915; f_acc: 0.9215\n",
            "(1 stylegan    ) acc: 96.5833; ap: 99.7426; r_acc: 1.0000; f_acc: 0.9317\n",
            "(2 stylegan2   ) acc: 93.6875; ap: 98.8653; r_acc: 0.9938; f_acc: 0.8800\n",
            "(3 biggan      ) acc: 82.2500; ap: 91.8575; r_acc: 0.7100; f_acc: 0.9350\n",
            "(4 cyclegan    ) acc: 80.2830; ap: 88.9809; r_acc: 0.8491; f_acc: 0.7566\n",
            "(5 stargan     ) acc: 88.7500; ap: 99.9562; r_acc: 1.0000; f_acc: 0.7750\n",
            "(6 gaugan      ) acc: 74.7500; ap: 75.9286; r_acc: 0.5600; f_acc: 0.9350\n",
            "(7 deepfake    ) acc: 58.2500; ap: 88.3084; r_acc: 0.9950; f_acc: 0.1700\n",
            "(8 Mean      ) acc: 83.7755; ap: 92.8760\n",
            "*************************\n",
            "2025_03_10_08_03_05\n",
            "2025_03_10_08_03_25 Train loss/d_loss: 0.0017405115067958832/1.6659899950027466 at step: 4550 lr 0.000162\n",
            "2025_03_10_08_03_44 Train loss/d_loss: 0.004881964530795813/1.6358284950256348 at step: 4600 lr 0.000162\n",
            "2025_03_10_08_04_04 Train loss/d_loss: 0.02728678472340107/1.6642937660217285 at step: 4650 lr 0.000162\n",
            "2025_03_10_08_04_24 Train loss/d_loss: 0.03833305463194847/1.769736647605896 at step: 4700 lr 0.000162\n",
            "2025_03_10_08_04_43 Train loss/d_loss: 0.0055094314739108086/1.6657309532165527 at step: 4750 lr 0.000162\n",
            "2025_03_10_08_05_03 Train loss/d_loss: 0.008223561570048332/1.7110154628753662 at step: 4800 lr 0.000162\n",
            "2025_03_10_08_05_22 Train loss/d_loss: 0.005124617367982864/1.6695711612701416 at step: 4850 lr 0.000162\n",
            "Saving model ./checkpoints/adof_distill-alpha_02025_03_10_07_18_34/model_epoch_12.pth\n",
            " Epoch 12 : 2025_03_10_08_03_05 --> 2025_03_10_08_05_32\n",
            "Saving model ./checkpoints/adof_distill-alpha_02025_03_10_07_18_34/model_epoch_last.pth\n",
            "(Val @ epoch 12) acc: 0.9825; ap: 0.9986699335409185\n",
            "*************************\n",
            "2025_03_10_08_05_38\n",
            "(0 progan      ) acc: 96.7875; ap: 99.6689; r_acc: 0.9850; f_acc: 0.9507\n",
            "(1 stylegan    ) acc: 92.0000; ap: 99.0344; r_acc: 0.9800; f_acc: 0.8600\n",
            "(2 stylegan2   ) acc: 97.5000; ap: 99.7454; r_acc: 0.9800; f_acc: 0.9700\n",
            "(3 biggan      ) acc: 78.2500; ap: 88.4875; r_acc: 0.9250; f_acc: 0.6400\n",
            "(4 cyclegan    ) acc: 60.3774; ap: 87.3940; r_acc: 0.9849; f_acc: 0.2226\n",
            "(5 stargan     ) acc: 64.0000; ap: 92.5235; r_acc: 0.9900; f_acc: 0.2900\n",
            "(6 gaugan      ) acc: 58.5000; ap: 69.0439; r_acc: 0.9050; f_acc: 0.2650\n",
            "(7 deepfake    ) acc: 56.0000; ap: 79.7615; r_acc: 0.9800; f_acc: 0.1400\n",
            "(8 Mean      ) acc: 75.4269; ap: 89.4574\n",
            "*************************\n",
            "2025_03_10_08_06_41\n",
            "2025_03_10_08_06_51 Train loss/d_loss: 0.003679688088595867/1.6638716459274292 at step: 4900 lr 0.000162\n",
            "2025_03_10_08_07_11 Train loss/d_loss: 0.003189587499946356/1.6744356155395508 at step: 4950 lr 0.000162\n",
            "2025_03_10_08_07_31 Train loss/d_loss: 0.0011234269477427006/1.659745693206787 at step: 5000 lr 0.000162\n",
            "2025_03_10_08_07_50 Train loss/d_loss: 0.003476897720247507/1.645675539970398 at step: 5050 lr 0.000162\n",
            "2025_03_10_08_08_10 Train loss/d_loss: 0.003310941159725189/1.6942287683486938 at step: 5100 lr 0.000162\n",
            "2025_03_10_08_08_29 Train loss/d_loss: 0.01683451235294342/1.689220666885376 at step: 5150 lr 0.000162\n",
            "2025_03_10_08_08_49 Train loss/d_loss: 0.026486819609999657/1.7139883041381836 at step: 5200 lr 0.000162\n",
            "2025_03_10_08_09_08 Train loss/d_loss: 0.011632212437689304/1.686598300933838 at step: 5250 lr 0.000162\n",
            "Saving model ./checkpoints/adof_distill-alpha_02025_03_10_07_18_34/model_epoch_13.pth\n",
            " Epoch 13 : 2025_03_10_08_06_41 --> 2025_03_10_08_09_09\n",
            "Saving model ./checkpoints/adof_distill-alpha_02025_03_10_07_18_34/model_epoch_last.pth\n",
            "(Val @ epoch 13) acc: 0.96375; ap: 0.9978342509431554\n",
            "*************************\n",
            "2025_03_10_08_09_14\n",
            "(0 progan      ) acc: 96.2125; ap: 99.4797; r_acc: 0.9447; f_acc: 0.9795\n",
            "(1 stylegan    ) acc: 96.7500; ap: 99.6781; r_acc: 0.9500; f_acc: 0.9850\n",
            "(2 stylegan2   ) acc: 92.1875; ap: 98.0514; r_acc: 0.9337; f_acc: 0.9100\n",
            "(3 biggan      ) acc: 70.5000; ap: 82.5362; r_acc: 0.4350; f_acc: 0.9750\n",
            "(4 cyclegan    ) acc: 77.3585; ap: 83.9189; r_acc: 0.6528; f_acc: 0.8943\n",
            "(5 stargan     ) acc: 89.2500; ap: 99.9543; r_acc: 0.7850; f_acc: 1.0000\n",
            "(6 gaugan      ) acc: 63.2500; ap: 67.4367; r_acc: 0.2650; f_acc: 1.0000\n",
            "(7 deepfake    ) acc: 74.2500; ap: 93.7046; r_acc: 0.5000; f_acc: 0.9850\n",
            "(8 Mean      ) acc: 82.4698; ap: 90.5950\n",
            "*************************\n",
            "2025_03_10_08_10_17\n",
            "2025_03_10_08_10_37 Train loss/d_loss: 0.002221573144197464/1.7089271545410156 at step: 5300 lr 0.000162\n",
            "2025_03_10_08_10_57 Train loss/d_loss: 0.0025558883789926767/1.6880624294281006 at step: 5350 lr 0.000162\n",
            "2025_03_10_08_11_17 Train loss/d_loss: 0.003771997056901455/1.6528189182281494 at step: 5400 lr 0.000162\n",
            "2025_03_10_08_11_36 Train loss/d_loss: 0.0018466460751369596/1.653128743171692 at step: 5450 lr 0.000162\n",
            "2025_03_10_08_11_56 Train loss/d_loss: 0.005709420889616013/1.6416988372802734 at step: 5500 lr 0.000162\n",
            "2025_03_10_08_12_15 Train loss/d_loss: 0.029359575361013412/1.6679952144622803 at step: 5550 lr 0.000162\n",
            "2025_03_10_08_12_35 Train loss/d_loss: 0.003925231751054525/1.7272915840148926 at step: 5600 lr 0.000162\n",
            "Saving model ./checkpoints/adof_distill-alpha_02025_03_10_07_18_34/model_epoch_14.pth\n",
            " Epoch 14 : 2025_03_10_08_10_17 --> 2025_03_10_08_12_45\n",
            "Saving model ./checkpoints/adof_distill-alpha_02025_03_10_07_18_34/model_epoch_last.pth\n",
            "(Val @ epoch 14) acc: 0.504375; ap: 0.9528520342098958\n",
            "*************************\n",
            "2025_03_10_08_12_51\n",
            "(0 progan      ) acc: 50.9750; ap: 94.3675; r_acc: 1.0000; f_acc: 0.0195\n",
            "(1 stylegan    ) acc: 51.6667; ap: 97.6816; r_acc: 1.0000; f_acc: 0.0333\n",
            "(2 stylegan2   ) acc: 50.8750; ap: 93.8095; r_acc: 1.0000; f_acc: 0.0175\n",
            "(3 biggan      ) acc: 53.5000; ap: 76.5774; r_acc: 0.9950; f_acc: 0.0750\n",
            "(4 cyclegan    ) acc: 53.4906; ap: 91.4452; r_acc: 1.0000; f_acc: 0.0698\n",
            "(5 stargan     ) acc: 50.5000; ap: 99.3019; r_acc: 1.0000; f_acc: 0.0100\n",
            "(6 gaugan      ) acc: 53.2500; ap: 86.8103; r_acc: 1.0000; f_acc: 0.0650\n",
            "(7 deepfake    ) acc: 50.0000; ap: 75.2132; r_acc: 1.0000; f_acc: 0.0000\n",
            "(8 Mean      ) acc: 51.7822; ap: 89.4008\n",
            "*************************\n",
            "2025_03_10_08_13_53\n",
            "2025_03_10_08_14_03 Train loss/d_loss: 0.010996317490935326/1.6959197521209717 at step: 5650 lr 0.000162\n",
            "2025_03_10_08_14_23 Train loss/d_loss: 0.005104880314320326/1.707204818725586 at step: 5700 lr 0.000162\n",
            "2025_03_10_08_14_43 Train loss/d_loss: 0.003969209268689156/1.602529764175415 at step: 5750 lr 0.000162\n",
            "2025_03_10_08_15_02 Train loss/d_loss: 0.01447281800210476/1.7199773788452148 at step: 5800 lr 0.000162\n",
            "2025_03_10_08_15_22 Train loss/d_loss: 0.03138071298599243/1.6786634922027588 at step: 5850 lr 0.000162\n",
            "2025_03_10_08_15_42 Train loss/d_loss: 0.030058901757001877/1.6423466205596924 at step: 5900 lr 0.000162\n",
            "2025_03_10_08_16_01 Train loss/d_loss: 0.006273457780480385/1.6561226844787598 at step: 5950 lr 0.000162\n",
            "2025_03_10_08_16_21 Train loss/d_loss: 0.012621976435184479/1.6840583086013794 at step: 6000 lr 0.000162\n",
            "2025_03_10_08_16_21 changing lr at the end of epoch 15, iters 6000\n",
            "*************************\n",
            "Changing lr from 0.000162 to 0.00014580000000000002\n",
            "*************************\n",
            "Saving model ./checkpoints/adof_distill-alpha_02025_03_10_07_18_34/model_epoch_15.pth\n",
            " Epoch 15 : 2025_03_10_08_13_53 --> 2025_03_10_08_16_21\n",
            "Saving model ./checkpoints/adof_distill-alpha_02025_03_10_07_18_34/model_epoch_last.pth\n",
            "(Val @ epoch 15) acc: 0.905; ap: 0.9992965437145984\n",
            "*************************\n",
            "2025_03_10_08_16_27\n",
            "(0 progan      ) acc: 87.3375; ap: 99.7969; r_acc: 0.9995; f_acc: 0.7472\n",
            "(1 stylegan    ) acc: 82.5833; ap: 99.4286; r_acc: 1.0000; f_acc: 0.6517\n",
            "(2 stylegan2   ) acc: 87.1250; ap: 99.4463; r_acc: 1.0000; f_acc: 0.7425\n",
            "(3 biggan      ) acc: 64.2500; ap: 87.9045; r_acc: 0.9900; f_acc: 0.2950\n",
            "(4 cyclegan    ) acc: 54.7170; ap: 89.2991; r_acc: 1.0000; f_acc: 0.0943\n",
            "(5 stargan     ) acc: 56.5000; ap: 95.5565; r_acc: 1.0000; f_acc: 0.1300\n",
            "(6 gaugan      ) acc: 52.5000; ap: 76.8379; r_acc: 0.9750; f_acc: 0.0750\n",
            "(7 deepfake    ) acc: 54.0000; ap: 79.9611; r_acc: 0.9950; f_acc: 0.0850\n",
            "(8 Mean      ) acc: 67.3766; ap: 91.0289\n",
            "*************************\n",
            "2025_03_10_08_17_29\n",
            "2025_03_10_08_17_49 Train loss/d_loss: 0.0013080209027975798/1.6460013389587402 at step: 6050 lr 0.00014580000000000002\n",
            "2025_03_10_08_18_09 Train loss/d_loss: 0.035295519977808/1.673192024230957 at step: 6100 lr 0.00014580000000000002\n",
            "2025_03_10_08_18_28 Train loss/d_loss: 0.012130807153880596/1.7125097513198853 at step: 6150 lr 0.00014580000000000002\n",
            "2025_03_10_08_18_48 Train loss/d_loss: 0.0016526191029697657/1.6644104719161987 at step: 6200 lr 0.00014580000000000002\n",
            "2025_03_10_08_19_08 Train loss/d_loss: 0.016096215695142746/1.7021334171295166 at step: 6250 lr 0.00014580000000000002\n",
            "2025_03_10_08_19_27 Train loss/d_loss: 0.005299760028719902/1.6998560428619385 at step: 6300 lr 0.00014580000000000002\n",
            "2025_03_10_08_19_47 Train loss/d_loss: 0.008079659193754196/1.6958110332489014 at step: 6350 lr 0.00014580000000000002\n",
            "Saving model ./checkpoints/adof_distill-alpha_02025_03_10_07_18_34/model_epoch_16.pth\n",
            " Epoch 16 : 2025_03_10_08_17_29 --> 2025_03_10_08_19_57\n",
            "Saving model ./checkpoints/adof_distill-alpha_02025_03_10_07_18_34/model_epoch_last.pth\n",
            "(Val @ epoch 16) acc: 0.896875; ap: 0.9987547519270363\n",
            "*************************\n",
            "2025_03_10_08_20_04\n",
            "(0 progan      ) acc: 87.4500; ap: 99.4775; r_acc: 0.9985; f_acc: 0.7505\n",
            "(1 stylegan    ) acc: 84.6667; ap: 99.8270; r_acc: 1.0000; f_acc: 0.6933\n",
            "(2 stylegan2   ) acc: 85.0625; ap: 99.1886; r_acc: 0.9988; f_acc: 0.7025\n",
            "(3 biggan      ) acc: 77.0000; ap: 88.5209; r_acc: 0.9300; f_acc: 0.6100\n",
            "(4 cyclegan    ) acc: 68.8208; ap: 91.3584; r_acc: 0.9774; f_acc: 0.3991\n",
            "(5 stargan     ) acc: 62.0000; ap: 98.8632; r_acc: 1.0000; f_acc: 0.2400\n",
            "(6 gaugan      ) acc: 64.0000; ap: 76.4787; r_acc: 0.8700; f_acc: 0.4100\n",
            "(7 deepfake    ) acc: 51.2500; ap: 72.9227; r_acc: 0.9950; f_acc: 0.0300\n",
            "(8 Mean      ) acc: 72.5312; ap: 90.8296\n",
            "*************************\n",
            "2025_03_10_08_21_06\n",
            "2025_03_10_08_21_17 Train loss/d_loss: 0.0026150494813919067/1.6216819286346436 at step: 6400 lr 0.00014580000000000002\n",
            "2025_03_10_08_21_36 Train loss/d_loss: 0.002170973690226674/1.7205675840377808 at step: 6450 lr 0.00014580000000000002\n",
            "2025_03_10_08_21_56 Train loss/d_loss: 0.007548878900706768/1.6863911151885986 at step: 6500 lr 0.00014580000000000002\n",
            "2025_03_10_08_22_16 Train loss/d_loss: 0.002325652167201042/1.681277871131897 at step: 6550 lr 0.00014580000000000002\n",
            "2025_03_10_08_22_35 Train loss/d_loss: 0.0047767143696546555/1.7115514278411865 at step: 6600 lr 0.00014580000000000002\n",
            "2025_03_10_08_22_55 Train loss/d_loss: 0.1008409634232521/1.69400954246521 at step: 6650 lr 0.00014580000000000002\n",
            "2025_03_10_08_23_15 Train loss/d_loss: 0.003400511108338833/1.6324422359466553 at step: 6700 lr 0.00014580000000000002\n",
            "2025_03_10_08_23_34 Train loss/d_loss: 0.0011626780033111572/1.7204999923706055 at step: 6750 lr 0.00014580000000000002\n",
            "Saving model ./checkpoints/adof_distill-alpha_02025_03_10_07_18_34/model_epoch_17.pth\n",
            " Epoch 17 : 2025_03_10_08_21_06 --> 2025_03_10_08_23_34\n",
            "Saving model ./checkpoints/adof_distill-alpha_02025_03_10_07_18_34/model_epoch_last.pth\n",
            "(Val @ epoch 17) acc: 0.905; ap: 0.9998363494071968\n",
            "*************************\n",
            "2025_03_10_08_23_41\n",
            "(0 progan      ) acc: 89.0750; ap: 99.8842; r_acc: 0.9998; f_acc: 0.7817\n",
            "(1 stylegan    ) acc: 87.0833; ap: 99.9803; r_acc: 1.0000; f_acc: 0.7417\n",
            "(2 stylegan2   ) acc: 86.0625; ap: 99.4254; r_acc: 1.0000; f_acc: 0.7212\n",
            "(3 biggan      ) acc: 73.5000; ap: 92.4536; r_acc: 0.9900; f_acc: 0.4800\n",
            "(4 cyclegan    ) acc: 60.9906; ap: 91.1362; r_acc: 0.9943; f_acc: 0.2255\n",
            "(5 stargan     ) acc: 69.0000; ap: 99.6443; r_acc: 1.0000; f_acc: 0.3800\n",
            "(6 gaugan      ) acc: 62.7500; ap: 85.0670; r_acc: 0.9550; f_acc: 0.3000\n",
            "(7 deepfake    ) acc: 55.7500; ap: 83.6383; r_acc: 0.9800; f_acc: 0.1350\n",
            "(8 Mean      ) acc: 73.0264; ap: 93.9037\n",
            "*************************\n",
            "2025_03_10_08_24_43\n",
            "2025_03_10_08_25_03 Train loss/d_loss: 0.0030252200085669756/1.6345065832138062 at step: 6800 lr 0.00014580000000000002\n",
            "2025_03_10_08_25_23 Train loss/d_loss: 0.02153998427093029/1.6813786029815674 at step: 6850 lr 0.00014580000000000002\n",
            "2025_03_10_08_25_42 Train loss/d_loss: 0.001665016752667725/1.6536331176757812 at step: 6900 lr 0.00014580000000000002\n",
            "2025_03_10_08_26_02 Train loss/d_loss: 0.02564128488302231/1.6013484001159668 at step: 6950 lr 0.00014580000000000002\n",
            "2025_03_10_08_26_22 Train loss/d_loss: 0.0007524001994170249/1.7203657627105713 at step: 7000 lr 0.00014580000000000002\n",
            "2025_03_10_08_26_41 Train loss/d_loss: 0.004759503528475761/1.6113910675048828 at step: 7050 lr 0.00014580000000000002\n",
            "2025_03_10_08_27_01 Train loss/d_loss: 0.005739645101130009/1.6733927726745605 at step: 7100 lr 0.00014580000000000002\n",
            "Saving model ./checkpoints/adof_distill-alpha_02025_03_10_07_18_34/model_epoch_18.pth\n",
            " Epoch 18 : 2025_03_10_08_24_43 --> 2025_03_10_08_27_11\n",
            "Saving model ./checkpoints/adof_distill-alpha_02025_03_10_07_18_34/model_epoch_last.pth\n",
            "(Val @ epoch 18) acc: 0.979375; ap: 0.9991459604887988\n",
            "*************************\n",
            "2025_03_10_08_27_18\n",
            "(0 progan      ) acc: 96.6375; ap: 99.6583; r_acc: 0.9470; f_acc: 0.9858\n",
            "(1 stylegan    ) acc: 97.9167; ap: 99.8188; r_acc: 0.9750; f_acc: 0.9833\n",
            "(2 stylegan2   ) acc: 95.3750; ap: 99.1940; r_acc: 0.9437; f_acc: 0.9637\n",
            "(3 biggan      ) acc: 77.0000; ap: 88.8345; r_acc: 0.5800; f_acc: 0.9600\n",
            "(4 cyclegan    ) acc: 70.2830; ap: 80.9130; r_acc: 0.7462; f_acc: 0.6594\n",
            "(5 stargan     ) acc: 96.2500; ap: 99.2639; r_acc: 0.9850; f_acc: 0.9400\n",
            "(6 gaugan      ) acc: 65.5000; ap: 59.4798; r_acc: 0.4100; f_acc: 0.9000\n",
            "(7 deepfake    ) acc: 70.0000; ap: 85.7720; r_acc: 0.9300; f_acc: 0.4700\n",
            "(8 Mean      ) acc: 83.6203; ap: 89.1168\n",
            "*************************\n",
            "2025_03_10_08_28_19\n",
            "2025_03_10_08_28_29 Train loss/d_loss: 0.0038972366601228714/1.6756033897399902 at step: 7150 lr 0.00014580000000000002\n",
            "2025_03_10_08_28_49 Train loss/d_loss: 0.007548095192760229/1.6637992858886719 at step: 7200 lr 0.00014580000000000002\n",
            "2025_03_10_08_29_09 Train loss/d_loss: 0.04802611470222473/1.700984239578247 at step: 7250 lr 0.00014580000000000002\n",
            "2025_03_10_08_29_28 Train loss/d_loss: 0.00264465669170022/1.674759864807129 at step: 7300 lr 0.00014580000000000002\n",
            "2025_03_10_08_29_48 Train loss/d_loss: 0.01596376672387123/1.642432689666748 at step: 7350 lr 0.00014580000000000002\n",
            "2025_03_10_08_30_08 Train loss/d_loss: 0.0036027859896421432/1.733256459236145 at step: 7400 lr 0.00014580000000000002\n",
            "2025_03_10_08_30_27 Train loss/d_loss: 0.009694268926978111/1.6789807081222534 at step: 7450 lr 0.00014580000000000002\n",
            "2025_03_10_08_30_47 Train loss/d_loss: 0.00521929981186986/1.6924998760223389 at step: 7500 lr 0.00014580000000000002\n",
            "Saving model ./checkpoints/adof_distill-alpha_02025_03_10_07_18_34/model_epoch_19.pth\n",
            " Epoch 19 : 2025_03_10_08_28_19 --> 2025_03_10_08_30_47\n",
            "Saving model ./checkpoints/adof_distill-alpha_02025_03_10_07_18_34/model_epoch_last.pth\n",
            "(Val @ epoch 19) acc: 0.5425; ap: 0.9915145457051215\n",
            "*************************\n",
            "2025_03_10_08_30_54\n",
            "(0 progan      ) acc: 55.4875; ap: 98.7954; r_acc: 0.1098; f_acc: 1.0000\n",
            "(1 stylegan    ) acc: 57.5000; ap: 97.0235; r_acc: 0.1500; f_acc: 1.0000\n",
            "(2 stylegan2   ) acc: 54.8750; ap: 97.8352; r_acc: 0.0975; f_acc: 1.0000\n",
            "(3 biggan      ) acc: 57.0000; ap: 73.2795; r_acc: 0.1400; f_acc: 1.0000\n",
            "(4 cyclegan    ) acc: 55.0000; ap: 58.8445; r_acc: 0.1887; f_acc: 0.9113\n",
            "(5 stargan     ) acc: 50.0000; ap: 82.7752; r_acc: 0.0000; f_acc: 1.0000\n",
            "(6 gaugan      ) acc: 53.2500; ap: 49.3414; r_acc: 0.0750; f_acc: 0.9900\n",
            "(7 deepfake    ) acc: 50.5000; ap: 65.0134; r_acc: 0.0100; f_acc: 1.0000\n",
            "(8 Mean      ) acc: 54.2016; ap: 77.8635\n",
            "*************************\n",
            "2025_03_10_08_31_54\n",
            "2025_03_10_08_32_14 Train loss/d_loss: 0.020306456834077835/1.687470555305481 at step: 7550 lr 0.00014580000000000002\n",
            "2025_03_10_08_32_34 Train loss/d_loss: 0.006018646061420441/1.6459918022155762 at step: 7600 lr 0.00014580000000000002\n",
            "2025_03_10_08_32_54 Train loss/d_loss: 0.007318681105971336/1.6169774532318115 at step: 7650 lr 0.00014580000000000002\n",
            "2025_03_10_08_33_13 Train loss/d_loss: 0.01653803512454033/1.6977849006652832 at step: 7700 lr 0.00014580000000000002\n",
            "2025_03_10_08_33_33 Train loss/d_loss: 0.006511282175779343/1.646350383758545 at step: 7750 lr 0.00014580000000000002\n",
            "2025_03_10_08_33_52 Train loss/d_loss: 0.028877345845103264/1.6388874053955078 at step: 7800 lr 0.00014580000000000002\n",
            "2025_03_10_08_34_12 Train loss/d_loss: 0.02063070796430111/1.6662026643753052 at step: 7850 lr 0.00014580000000000002\n",
            "2025_03_10_08_34_22 changing lr at the end of epoch 20, iters 7875\n",
            "*************************\n",
            "Changing lr from 0.00014580000000000005 to 0.00013122000000000003\n",
            "*************************\n",
            "Saving model ./checkpoints/adof_distill-alpha_02025_03_10_07_18_34/model_epoch_20.pth\n",
            " Epoch 20 : 2025_03_10_08_31_54 --> 2025_03_10_08_34_22\n",
            "Saving model ./checkpoints/adof_distill-alpha_02025_03_10_07_18_34/model_epoch_last.pth\n",
            "(Val @ epoch 20) acc: 0.9975; ap: 0.9999891322890819\n",
            "*************************\n",
            "2025_03_10_08_34_28\n",
            "(0 progan      ) acc: 98.9250; ap: 99.9697; r_acc: 0.9982; f_acc: 0.9802\n",
            "(1 stylegan    ) acc: 97.8333; ap: 99.9876; r_acc: 1.0000; f_acc: 0.9567\n",
            "(2 stylegan2   ) acc: 96.7500; ap: 99.6772; r_acc: 0.9988; f_acc: 0.9363\n",
            "(3 biggan      ) acc: 79.7500; ap: 89.2145; r_acc: 0.9050; f_acc: 0.6900\n",
            "(4 cyclegan    ) acc: 63.7264; ap: 88.8921; r_acc: 0.9717; f_acc: 0.3028\n",
            "(5 stargan     ) acc: 89.5000; ap: 99.0824; r_acc: 0.9900; f_acc: 0.8000\n",
            "(6 gaugan      ) acc: 64.5000; ap: 70.6332; r_acc: 0.7950; f_acc: 0.4950\n",
            "(7 deepfake    ) acc: 84.7500; ap: 89.9084; r_acc: 0.8550; f_acc: 0.8400\n",
            "(8 Mean      ) acc: 84.4668; ap: 92.1706\n",
            "*************************\n",
            "2025_03_10_08_35_30\n",
            "2025_03_10_08_35_40 Train loss/d_loss: 0.0026534521020948887/1.6469743251800537 at step: 7900 lr 0.00013122000000000003\n",
            "2025_03_10_08_36_00 Train loss/d_loss: 0.0034290426410734653/1.6614770889282227 at step: 7950 lr 0.00013122000000000003\n",
            "2025_03_10_08_36_20 Train loss/d_loss: 0.005314827896654606/1.7111568450927734 at step: 8000 lr 0.00013122000000000003\n",
            "2025_03_10_08_36_39 Train loss/d_loss: 0.001870251027867198/1.7511181831359863 at step: 8050 lr 0.00013122000000000003\n",
            "2025_03_10_08_36_59 Train loss/d_loss: 0.0016388921067118645/1.6702947616577148 at step: 8100 lr 0.00013122000000000003\n",
            "2025_03_10_08_37_18 Train loss/d_loss: 0.0015671004075556993/1.6854113340377808 at step: 8150 lr 0.00013122000000000003\n",
            "2025_03_10_08_37_38 Train loss/d_loss: 0.0059449328109622/1.7040969133377075 at step: 8200 lr 0.00013122000000000003\n",
            "2025_03_10_08_37_58 Train loss/d_loss: 0.04281456768512726/1.7076642513275146 at step: 8250 lr 0.00013122000000000003\n",
            "Saving model ./checkpoints/adof_distill-alpha_02025_03_10_07_18_34/model_epoch_21.pth\n",
            " Epoch 21 : 2025_03_10_08_35_30 --> 2025_03_10_08_37_58\n",
            "Saving model ./checkpoints/adof_distill-alpha_02025_03_10_07_18_34/model_epoch_last.pth\n",
            "(Val @ epoch 21) acc: 0.88125; ap: 0.9998334716443555\n",
            "*************************\n",
            "2025_03_10_08_38_04\n",
            "(0 progan      ) acc: 86.4625; ap: 99.8339; r_acc: 1.0000; f_acc: 0.7292\n",
            "(1 stylegan    ) acc: 77.8333; ap: 99.8279; r_acc: 1.0000; f_acc: 0.5567\n",
            "(2 stylegan2   ) acc: 83.3125; ap: 99.2276; r_acc: 1.0000; f_acc: 0.6663\n",
            "(3 biggan      ) acc: 61.5000; ap: 89.8226; r_acc: 0.9950; f_acc: 0.2350\n",
            "(4 cyclegan    ) acc: 54.7642; ap: 90.9763; r_acc: 1.0000; f_acc: 0.0953\n",
            "(5 stargan     ) acc: 60.2500; ap: 97.4632; r_acc: 1.0000; f_acc: 0.2050\n",
            "(6 gaugan      ) acc: 53.7500; ap: 82.7538; r_acc: 0.9950; f_acc: 0.0800\n",
            "(7 deepfake    ) acc: 51.2500; ap: 75.0883; r_acc: 0.9850; f_acc: 0.0400\n",
            "(8 Mean      ) acc: 66.1403; ap: 91.8742\n",
            "*************************\n",
            "2025_03_10_08_39_06\n",
            "2025_03_10_08_39_26 Train loss/d_loss: 0.00421462906524539/1.6755616664886475 at step: 8300 lr 0.00013122000000000003\n",
            "2025_03_10_08_39_46 Train loss/d_loss: 0.0013602925464510918/1.6894519329071045 at step: 8350 lr 0.00013122000000000003\n",
            "2025_03_10_08_40_05 Train loss/d_loss: 0.006821946706622839/1.6976454257965088 at step: 8400 lr 0.00013122000000000003\n",
            "2025_03_10_08_40_25 Train loss/d_loss: 0.026717882603406906/1.7240188121795654 at step: 8450 lr 0.00013122000000000003\n",
            "2025_03_10_08_40_45 Train loss/d_loss: 0.0021616548765450716/1.6482850313186646 at step: 8500 lr 0.00013122000000000003\n",
            "2025_03_10_08_41_04 Train loss/d_loss: 0.01261108461767435/1.6858813762664795 at step: 8550 lr 0.00013122000000000003\n",
            "2025_03_10_08_41_24 Train loss/d_loss: 0.002491702325642109/1.6166621446609497 at step: 8600 lr 0.00013122000000000003\n",
            "Saving model ./checkpoints/adof_distill-alpha_02025_03_10_07_18_34/model_epoch_22.pth\n",
            " Epoch 22 : 2025_03_10_08_39_06 --> 2025_03_10_08_41_34\n",
            "Saving model ./checkpoints/adof_distill-alpha_02025_03_10_07_18_34/model_epoch_last.pth\n",
            "(Val @ epoch 22) acc: 0.77; ap: 0.9991857006456527\n",
            "*************************\n",
            "2025_03_10_08_41_40\n",
            "(0 progan      ) acc: 78.1875; ap: 99.7231; r_acc: 1.0000; f_acc: 0.5637\n",
            "(1 stylegan    ) acc: 74.8333; ap: 99.7133; r_acc: 1.0000; f_acc: 0.4967\n",
            "(2 stylegan2   ) acc: 77.3125; ap: 98.6719; r_acc: 1.0000; f_acc: 0.5463\n",
            "(3 biggan      ) acc: 58.5000; ap: 86.8531; r_acc: 0.9850; f_acc: 0.1850\n",
            "(4 cyclegan    ) acc: 52.5472; ap: 88.7967; r_acc: 0.9991; f_acc: 0.0519\n",
            "(5 stargan     ) acc: 63.7500; ap: 97.5261; r_acc: 1.0000; f_acc: 0.2750\n",
            "(6 gaugan      ) acc: 52.2500; ap: 83.8234; r_acc: 0.9950; f_acc: 0.0500\n",
            "(7 deepfake    ) acc: 63.2500; ap: 85.3611; r_acc: 0.9600; f_acc: 0.3050\n",
            "(8 Mean      ) acc: 65.0788; ap: 92.5586\n",
            "*************************\n",
            "2025_03_10_08_42_41\n",
            "2025_03_10_08_42_52 Train loss/d_loss: 0.0004776991263497621/1.6924490928649902 at step: 8650 lr 0.00013122000000000003\n",
            "2025_03_10_08_43_11 Train loss/d_loss: 0.02097637578845024/1.6355856657028198 at step: 8700 lr 0.00013122000000000003\n",
            "2025_03_10_08_43_31 Train loss/d_loss: 0.000127687249914743/1.7525066137313843 at step: 8750 lr 0.00013122000000000003\n",
            "2025_03_10_08_43_50 Train loss/d_loss: 0.0018132247496396303/1.5849201679229736 at step: 8800 lr 0.00013122000000000003\n",
            "2025_03_10_08_44_10 Train loss/d_loss: 0.00024893999216146767/1.7692453861236572 at step: 8850 lr 0.00013122000000000003\n",
            "2025_03_10_08_44_30 Train loss/d_loss: 0.08216022700071335/1.699004888534546 at step: 8900 lr 0.00013122000000000003\n",
            "2025_03_10_08_44_49 Train loss/d_loss: 0.0013333477545529604/1.7471317052841187 at step: 8950 lr 0.00013122000000000003\n",
            "2025_03_10_08_45_09 Train loss/d_loss: 0.0008235147688537836/1.6584253311157227 at step: 9000 lr 0.00013122000000000003\n",
            "Saving model ./checkpoints/adof_distill-alpha_02025_03_10_07_18_34/model_epoch_23.pth\n",
            " Epoch 23 : 2025_03_10_08_42_41 --> 2025_03_10_08_45_09\n",
            "Saving model ./checkpoints/adof_distill-alpha_02025_03_10_07_18_34/model_epoch_last.pth\n",
            "(Val @ epoch 23) acc: 0.9875; ap: 0.9992749599214092\n",
            "*************************\n",
            "2025_03_10_08_45_15\n",
            "(0 progan      ) acc: 97.4375; ap: 99.7850; r_acc: 0.9965; f_acc: 0.9523\n",
            "(1 stylegan    ) acc: 98.0000; ap: 99.9514; r_acc: 1.0000; f_acc: 0.9600\n",
            "(2 stylegan2   ) acc: 93.3750; ap: 99.0285; r_acc: 0.9938; f_acc: 0.8738\n",
            "(3 biggan      ) acc: 86.2500; ap: 91.8571; r_acc: 0.8300; f_acc: 0.8950\n",
            "(4 cyclegan    ) acc: 75.0472; ap: 90.7521; r_acc: 0.9358; f_acc: 0.5651\n",
            "(5 stargan     ) acc: 96.2500; ap: 99.4459; r_acc: 0.9900; f_acc: 0.9350\n",
            "(6 gaugan      ) acc: 71.7500; ap: 75.5102; r_acc: 0.7150; f_acc: 0.7200\n",
            "(7 deepfake    ) acc: 79.0000; ap: 90.3981; r_acc: 0.9200; f_acc: 0.6600\n",
            "(8 Mean      ) acc: 87.1387; ap: 93.3410\n",
            "*************************\n",
            "2025_03_10_08_46_17\n",
            "2025_03_10_08_46_37 Train loss/d_loss: 0.0010408505331724882/1.6300325393676758 at step: 9050 lr 0.00013122000000000003\n",
            "2025_03_10_08_46_57 Train loss/d_loss: 0.004786340054124594/1.6655961275100708 at step: 9100 lr 0.00013122000000000003\n",
            "2025_03_10_08_47_16 Train loss/d_loss: 0.01131314504891634/1.6524115800857544 at step: 9150 lr 0.00013122000000000003\n",
            "2025_03_10_08_47_36 Train loss/d_loss: 0.0010534211760386825/1.691056728363037 at step: 9200 lr 0.00013122000000000003\n",
            "2025_03_10_08_47_55 Train loss/d_loss: 0.00040529342368245125/1.680314302444458 at step: 9250 lr 0.00013122000000000003\n",
            "2025_03_10_08_48_15 Train loss/d_loss: 0.001871412736363709/1.5978045463562012 at step: 9300 lr 0.00013122000000000003\n",
            "2025_03_10_08_48_35 Train loss/d_loss: 0.03713493421673775/1.6116092205047607 at step: 9350 lr 0.00013122000000000003\n",
            "Saving model ./checkpoints/adof_distill-alpha_02025_03_10_07_18_34/model_epoch_24.pth\n",
            " Epoch 24 : 2025_03_10_08_46_17 --> 2025_03_10_08_48_44\n",
            "Saving model ./checkpoints/adof_distill-alpha_02025_03_10_07_18_34/model_epoch_last.pth\n",
            "(Val @ epoch 24) acc: 0.9975; ap: 0.9999875948729864\n",
            "*************************\n",
            "2025_03_10_08_48_51\n",
            "(0 progan      ) acc: 98.3250; ap: 99.9229; r_acc: 0.9990; f_acc: 0.9675\n",
            "(1 stylegan    ) acc: 96.3333; ap: 99.9111; r_acc: 1.0000; f_acc: 0.9267\n",
            "(2 stylegan2   ) acc: 94.5625; ap: 99.3095; r_acc: 1.0000; f_acc: 0.8912\n",
            "(3 biggan      ) acc: 83.0000; ap: 91.1801; r_acc: 0.8900; f_acc: 0.7700\n",
            "(4 cyclegan    ) acc: 67.0755; ap: 88.6935; r_acc: 0.9679; f_acc: 0.3736\n",
            "(5 stargan     ) acc: 89.2500; ap: 98.4119; r_acc: 0.9850; f_acc: 0.8000\n",
            "(6 gaugan      ) acc: 62.7500; ap: 69.5294; r_acc: 0.7450; f_acc: 0.5100\n",
            "(7 deepfake    ) acc: 75.7500; ap: 86.8600; r_acc: 0.9050; f_acc: 0.6100\n",
            "(8 Mean      ) acc: 83.3808; ap: 91.7273\n",
            "*************************\n",
            "2025_03_10_08_49_52\n",
            "2025_03_10_08_50_02 Train loss/d_loss: 0.013893328607082367/1.664102554321289 at step: 9400 lr 0.00013122000000000003\n",
            "2025_03_10_08_50_22 Train loss/d_loss: 0.01248007733374834/1.6732523441314697 at step: 9450 lr 0.00013122000000000003\n",
            "2025_03_10_08_50_42 Train loss/d_loss: 0.0036336465273052454/1.703536868095398 at step: 9500 lr 0.00013122000000000003\n",
            "2025_03_10_08_51_01 Train loss/d_loss: 0.009449993260204792/1.6911548376083374 at step: 9550 lr 0.00013122000000000003\n",
            "2025_03_10_08_51_21 Train loss/d_loss: 0.003197174984961748/1.6442776918411255 at step: 9600 lr 0.00013122000000000003\n",
            "2025_03_10_08_51_40 Train loss/d_loss: 0.008064182475209236/1.6843669414520264 at step: 9650 lr 0.00013122000000000003\n",
            "2025_03_10_08_52_00 Train loss/d_loss: 0.009996641427278519/1.6371564865112305 at step: 9700 lr 0.00013122000000000003\n",
            "2025_03_10_08_52_19 Train loss/d_loss: 0.00033822027035057545/1.7320916652679443 at step: 9750 lr 0.00013122000000000003\n",
            "2025_03_10_08_52_20 changing lr at the end of epoch 25, iters 9750\n",
            "*************************\n",
            "Changing lr from 0.00013122000000000003 to 0.00011809800000000003\n",
            "*************************\n",
            "Saving model ./checkpoints/adof_distill-alpha_02025_03_10_07_18_34/model_epoch_25.pth\n",
            " Epoch 25 : 2025_03_10_08_49_52 --> 2025_03_10_08_52_20\n",
            "Saving model ./checkpoints/adof_distill-alpha_02025_03_10_07_18_34/model_epoch_last.pth\n",
            "(Val @ epoch 25) acc: 0.77375; ap: 0.998491302430363\n",
            "*************************\n",
            "2025_03_10_08_52_26\n",
            "(0 progan      ) acc: 79.3625; ap: 99.7895; r_acc: 0.5875; f_acc: 0.9998\n",
            "(1 stylegan    ) acc: 77.0000; ap: 99.1960; r_acc: 0.5400; f_acc: 1.0000\n",
            "(2 stylegan2   ) acc: 75.6250; ap: 99.0266; r_acc: 0.5138; f_acc: 0.9988\n",
            "(3 biggan      ) acc: 64.0000; ap: 76.6169; r_acc: 0.2850; f_acc: 0.9950\n",
            "(4 cyclegan    ) acc: 64.9057; ap: 69.7305; r_acc: 0.4443; f_acc: 0.8538\n",
            "(5 stargan     ) acc: 53.5000; ap: 90.1993; r_acc: 0.0700; f_acc: 1.0000\n",
            "(6 gaugan      ) acc: 56.2500; ap: 53.2736; r_acc: 0.1500; f_acc: 0.9750\n",
            "(7 deepfake    ) acc: 52.5000; ap: 72.5297; r_acc: 0.0550; f_acc: 0.9950\n",
            "(8 Mean      ) acc: 65.3929; ap: 82.5453\n",
            "*************************\n",
            "2025_03_10_08_53_28\n",
            "2025_03_10_08_53_48 Train loss/d_loss: 0.005748198367655277/1.6417632102966309 at step: 9800 lr 0.00011809800000000003\n",
            "2025_03_10_08_54_08 Train loss/d_loss: 0.0009226632537320256/1.72987961769104 at step: 9850 lr 0.00011809800000000003\n",
            "2025_03_10_08_54_27 Train loss/d_loss: 0.002815640065819025/1.6863062381744385 at step: 9900 lr 0.00011809800000000003\n",
            "2025_03_10_08_54_47 Train loss/d_loss: 0.002692046109586954/1.6455742120742798 at step: 9950 lr 0.00011809800000000003\n",
            "2025_03_10_08_55_06 Train loss/d_loss: 0.00027694174787029624/1.6474639177322388 at step: 10000 lr 0.00011809800000000003\n",
            "2025_03_10_08_55_26 Train loss/d_loss: 0.003443706315010786/1.6289892196655273 at step: 10050 lr 0.00011809800000000003\n",
            "2025_03_10_08_55_46 Train loss/d_loss: 0.0009600764024071395/1.6467787027359009 at step: 10100 lr 0.00011809800000000003\n",
            "Saving model ./checkpoints/adof_distill-alpha_02025_03_10_07_18_34/model_epoch_26.pth\n",
            " Epoch 26 : 2025_03_10_08_53_28 --> 2025_03_10_08_55_55\n",
            "Saving model ./checkpoints/adof_distill-alpha_02025_03_10_07_18_34/model_epoch_last.pth\n",
            "(Val @ epoch 26) acc: 0.99375; ap: 0.9997244311624999\n",
            "*************************\n",
            "2025_03_10_08_56_02\n",
            "(0 progan      ) acc: 98.5000; ap: 99.7898; r_acc: 0.9865; f_acc: 0.9835\n",
            "(1 stylegan    ) acc: 99.0833; ap: 99.9273; r_acc: 0.9950; f_acc: 0.9867\n",
            "(2 stylegan2   ) acc: 94.8125; ap: 99.1192; r_acc: 0.9825; f_acc: 0.9137\n",
            "(3 biggan      ) acc: 77.0000; ap: 86.3478; r_acc: 0.5750; f_acc: 0.9650\n",
            "(4 cyclegan    ) acc: 78.4434; ap: 86.0651; r_acc: 0.7849; f_acc: 0.7840\n",
            "(5 stargan     ) acc: 94.2500; ap: 99.8624; r_acc: 0.8850; f_acc: 1.0000\n",
            "(6 gaugan      ) acc: 66.5000; ap: 70.2648; r_acc: 0.3850; f_acc: 0.9450\n",
            "(7 deepfake    ) acc: 85.5000; ap: 90.4536; r_acc: 0.7750; f_acc: 0.9350\n",
            "(8 Mean      ) acc: 86.7612; ap: 91.4787\n",
            "*************************\n",
            "2025_03_10_08_57_04\n",
            "2025_03_10_08_57_15 Train loss/d_loss: 0.00391100300475955/1.6969306468963623 at step: 10150 lr 0.00011809800000000003\n",
            "2025_03_10_08_57_34 Train loss/d_loss: 0.0011395557085052133/1.6488597393035889 at step: 10200 lr 0.00011809800000000003\n",
            "2025_03_10_08_57_54 Train loss/d_loss: 0.0006883404566906393/1.623238444328308 at step: 10250 lr 0.00011809800000000003\n",
            "2025_03_10_08_58_14 Train loss/d_loss: 0.0008339312043972313/1.617469310760498 at step: 10300 lr 0.00011809800000000003\n",
            "2025_03_10_08_58_33 Train loss/d_loss: 0.0029913182370364666/1.7000703811645508 at step: 10350 lr 0.00011809800000000003\n",
            "2025_03_10_08_58_53 Train loss/d_loss: 0.002468391787260771/1.6872247457504272 at step: 10400 lr 0.00011809800000000003\n",
            "2025_03_10_08_59_12 Train loss/d_loss: 0.006458205170929432/1.625483512878418 at step: 10450 lr 0.00011809800000000003\n",
            "2025_03_10_08_59_32 Train loss/d_loss: 0.012777280993759632/1.65370512008667 at step: 10500 lr 0.00011809800000000003\n",
            "Saving model ./checkpoints/adof_distill-alpha_02025_03_10_07_18_34/model_epoch_27.pth\n",
            " Epoch 27 : 2025_03_10_08_57_04 --> 2025_03_10_08_59_32\n",
            "Saving model ./checkpoints/adof_distill-alpha_02025_03_10_07_18_34/model_epoch_last.pth\n",
            "(Val @ epoch 27) acc: 0.979375; ap: 0.9999828572200733\n",
            "*************************\n",
            "2025_03_10_08_59_39\n",
            "(0 progan      ) acc: 96.2750; ap: 99.9564; r_acc: 0.9995; f_acc: 0.9260\n",
            "(1 stylegan    ) acc: 89.0833; ap: 99.7025; r_acc: 1.0000; f_acc: 0.7817\n",
            "(2 stylegan2   ) acc: 93.5625; ap: 99.6678; r_acc: 0.9988; f_acc: 0.8725\n",
            "(3 biggan      ) acc: 72.2500; ap: 90.0024; r_acc: 0.9700; f_acc: 0.4750\n",
            "(4 cyclegan    ) acc: 58.9151; ap: 90.4941; r_acc: 0.9943; f_acc: 0.1840\n",
            "(5 stargan     ) acc: 62.2500; ap: 91.9243; r_acc: 0.9950; f_acc: 0.2500\n",
            "(6 gaugan      ) acc: 55.7500; ap: 74.2961; r_acc: 0.9650; f_acc: 0.1500\n",
            "(7 deepfake    ) acc: 55.2500; ap: 74.4823; r_acc: 0.9700; f_acc: 0.1350\n",
            "(8 Mean      ) acc: 72.9170; ap: 90.0657\n",
            "*************************\n",
            "2025_03_10_09_00_39\n",
            "2025_03_10_09_01_00 Train loss/d_loss: 0.001484859036281705/1.6584694385528564 at step: 10550 lr 0.00011809800000000003\n",
            "2025_03_10_09_01_20 Train loss/d_loss: 0.003955885302275419/1.628124713897705 at step: 10600 lr 0.00011809800000000003\n",
            "2025_03_10_09_01_39 Train loss/d_loss: 0.0003771094197873026/1.6751360893249512 at step: 10650 lr 0.00011809800000000003\n",
            "2025_03_10_09_01_59 Train loss/d_loss: 0.000876649864949286/1.6772143840789795 at step: 10700 lr 0.00011809800000000003\n",
            "2025_03_10_09_02_18 Train loss/d_loss: 0.006309637799859047/1.7181875705718994 at step: 10750 lr 0.00011809800000000003\n",
            "2025_03_10_09_02_38 Train loss/d_loss: 0.004020292777568102/1.6476033926010132 at step: 10800 lr 0.00011809800000000003\n",
            "2025_03_10_09_02_58 Train loss/d_loss: 0.011525025591254234/1.6567726135253906 at step: 10850 lr 0.00011809800000000003\n",
            "Saving model ./checkpoints/adof_distill-alpha_02025_03_10_07_18_34/model_epoch_28.pth\n",
            " Epoch 28 : 2025_03_10_09_00_39 --> 2025_03_10_09_03_08\n",
            "Saving model ./checkpoints/adof_distill-alpha_02025_03_10_07_18_34/model_epoch_last.pth\n",
            "(Val @ epoch 28) acc: 0.954375; ap: 0.9999178028263271\n",
            "*************************\n",
            "2025_03_10_09_03_14\n",
            "(0 progan      ) acc: 95.9875; ap: 99.9493; r_acc: 0.9215; f_acc: 0.9982\n",
            "(1 stylegan    ) acc: 95.0833; ap: 99.9162; r_acc: 0.9017; f_acc: 1.0000\n",
            "(2 stylegan2   ) acc: 93.4375; ap: 99.5621; r_acc: 0.8838; f_acc: 0.9850\n",
            "(3 biggan      ) acc: 74.0000; ap: 83.1531; r_acc: 0.5050; f_acc: 0.9750\n",
            "(4 cyclegan    ) acc: 72.5472; ap: 80.9239; r_acc: 0.7434; f_acc: 0.7075\n",
            "(5 stargan     ) acc: 76.7500; ap: 98.4465; r_acc: 0.5350; f_acc: 1.0000\n",
            "(6 gaugan      ) acc: 61.0000; ap: 59.0401; r_acc: 0.3050; f_acc: 0.9150\n",
            "(7 deepfake    ) acc: 72.5000; ap: 91.2704; r_acc: 0.4600; f_acc: 0.9900\n",
            "(8 Mean      ) acc: 80.1632; ap: 89.0327\n",
            "*************************\n",
            "2025_03_10_09_04_15\n",
            "2025_03_10_09_04_25 Train loss/d_loss: 0.0017650979571044445/1.6918545961380005 at step: 10900 lr 0.00011809800000000003\n",
            "2025_03_10_09_04_45 Train loss/d_loss: 0.0008260653121396899/1.7134389877319336 at step: 10950 lr 0.00011809800000000003\n",
            "2025_03_10_09_05_04 Train loss/d_loss: 0.00027241482166573405/1.7420717477798462 at step: 11000 lr 0.00011809800000000003\n",
            "2025_03_10_09_05_24 Train loss/d_loss: 0.0016801131423562765/1.671838402748108 at step: 11050 lr 0.00011809800000000003\n",
            "2025_03_10_09_05_43 Train loss/d_loss: 6.576688610948622e-05/1.680324673652649 at step: 11100 lr 0.00011809800000000003\n",
            "2025_03_10_09_06_03 Train loss/d_loss: 0.0015055816620588303/1.6754289865493774 at step: 11150 lr 0.00011809800000000003\n",
            "2025_03_10_09_06_23 Train loss/d_loss: 0.00551830418407917/1.663388967514038 at step: 11200 lr 0.00011809800000000003\n",
            "2025_03_10_09_06_42 Train loss/d_loss: 0.0003556955198291689/1.6794416904449463 at step: 11250 lr 0.00011809800000000003\n",
            "Saving model ./checkpoints/adof_distill-alpha_02025_03_10_07_18_34/model_epoch_29.pth\n",
            " Epoch 29 : 2025_03_10_09_04_15 --> 2025_03_10_09_06_43\n",
            "Saving model ./checkpoints/adof_distill-alpha_02025_03_10_07_18_34/model_epoch_last.pth\n",
            "(Val @ epoch 29) acc: 0.9975; ap: 0.9999890955403312\n",
            "*************************\n",
            "2025_03_10_09_06_48\n",
            "(0 progan      ) acc: 99.1000; ap: 99.9595; r_acc: 0.9965; f_acc: 0.9855\n",
            "(1 stylegan    ) acc: 97.9167; ap: 99.9759; r_acc: 1.0000; f_acc: 0.9583\n",
            "(2 stylegan2   ) acc: 96.1875; ap: 99.6244; r_acc: 0.9975; f_acc: 0.9263\n",
            "(3 biggan      ) acc: 86.0000; ap: 91.7085; r_acc: 0.8400; f_acc: 0.8800\n",
            "(4 cyclegan    ) acc: 72.2170; ap: 89.9599; r_acc: 0.9491; f_acc: 0.4953\n",
            "(5 stargan     ) acc: 85.0000; ap: 98.1061; r_acc: 0.9950; f_acc: 0.7050\n",
            "(6 gaugan      ) acc: 67.2500; ap: 71.3729; r_acc: 0.7800; f_acc: 0.5650\n",
            "(7 deepfake    ) acc: 60.2500; ap: 82.5782; r_acc: 0.9700; f_acc: 0.2350\n",
            "(8 Mean      ) acc: 82.9901; ap: 91.6607\n",
            "*************************\n",
            "2025_03_10_09_07_50\n",
            "2025_03_10_09_08_11 Train loss/d_loss: 0.0076551237143576145/1.6761212348937988 at step: 11300 lr 0.00011809800000000003\n",
            "2025_03_10_09_08_31 Train loss/d_loss: 0.03786252811551094/1.7140252590179443 at step: 11350 lr 0.00011809800000000003\n",
            "2025_03_10_09_08_50 Train loss/d_loss: 0.008047245442867279/1.6784250736236572 at step: 11400 lr 0.00011809800000000003\n",
            "2025_03_10_09_09_10 Train loss/d_loss: 0.015112913213670254/1.691012978553772 at step: 11450 lr 0.00011809800000000003\n",
            "2025_03_10_09_09_29 Train loss/d_loss: 0.0002610875526443124/1.7074977159500122 at step: 11500 lr 0.00011809800000000003\n",
            "2025_03_10_09_09_49 Train loss/d_loss: 0.0004335390403866768/1.6459650993347168 at step: 11550 lr 0.00011809800000000003\n",
            "2025_03_10_09_10_09 Train loss/d_loss: 0.0005131626967340708/1.6778944730758667 at step: 11600 lr 0.00011809800000000003\n",
            "2025_03_10_09_10_18 changing lr at the end of epoch 30, iters 11625\n",
            "*************************\n",
            "Changing lr from 0.00011809800000000003 to 0.00010628820000000004\n",
            "*************************\n",
            "Saving model ./checkpoints/adof_distill-alpha_02025_03_10_07_18_34/model_epoch_30.pth\n",
            " Epoch 30 : 2025_03_10_09_07_50 --> 2025_03_10_09_10_18\n",
            "Saving model ./checkpoints/adof_distill-alpha_02025_03_10_07_18_34/model_epoch_last.pth\n",
            "(Val @ epoch 30) acc: 0.986875; ap: 0.9998866501039863\n",
            "*************************\n",
            "2025_03_10_09_10_25\n",
            "(0 progan      ) acc: 96.6875; ap: 99.9117; r_acc: 0.9992; f_acc: 0.9345\n",
            "(1 stylegan    ) acc: 93.2500; ap: 99.9530; r_acc: 1.0000; f_acc: 0.8650\n",
            "(2 stylegan2   ) acc: 92.7500; ap: 99.5450; r_acc: 1.0000; f_acc: 0.8550\n",
            "(3 biggan      ) acc: 80.0000; ap: 93.1758; r_acc: 0.9600; f_acc: 0.6400\n",
            "(4 cyclegan    ) acc: 65.8019; ap: 91.9335; r_acc: 0.9906; f_acc: 0.3255\n",
            "(5 stargan     ) acc: 77.0000; ap: 99.6057; r_acc: 1.0000; f_acc: 0.5400\n",
            "(6 gaugan      ) acc: 61.2500; ap: 76.6705; r_acc: 0.9000; f_acc: 0.3250\n",
            "(7 deepfake    ) acc: 54.5000; ap: 80.8288; r_acc: 0.9800; f_acc: 0.1100\n",
            "(8 Mean      ) acc: 77.6549; ap: 92.7030\n",
            "*************************\n",
            "2025_03_10_09_11_26\n",
            "2025_03_10_09_11_36 Train loss/d_loss: 0.0056974138133227825/1.6652112007141113 at step: 11650 lr 0.00010628820000000004\n",
            "2025_03_10_09_11_56 Train loss/d_loss: 0.0035144693683832884/1.6363930702209473 at step: 11700 lr 0.00010628820000000004\n",
            "2025_03_10_09_12_16 Train loss/d_loss: 0.025516999885439873/1.6713383197784424 at step: 11750 lr 0.00010628820000000004\n",
            "2025_03_10_09_12_35 Train loss/d_loss: 0.0015983625780791044/1.7246828079223633 at step: 11800 lr 0.00010628820000000004\n",
            "2025_03_10_09_12_55 Train loss/d_loss: 0.005163480062037706/1.67649507522583 at step: 11850 lr 0.00010628820000000004\n",
            "2025_03_10_09_13_15 Train loss/d_loss: 0.011699900031089783/1.6591383218765259 at step: 11900 lr 0.00010628820000000004\n",
            "2025_03_10_09_13_34 Train loss/d_loss: 0.00023671481176279485/1.6166579723358154 at step: 11950 lr 0.00010628820000000004\n",
            "2025_03_10_09_13_54 Train loss/d_loss: 0.001304095727391541/1.6719460487365723 at step: 12000 lr 0.00010628820000000004\n",
            "Saving model ./checkpoints/adof_distill-alpha_02025_03_10_07_18_34/model_epoch_31.pth\n",
            " Epoch 31 : 2025_03_10_09_11_26 --> 2025_03_10_09_13_54\n",
            "Saving model ./checkpoints/adof_distill-alpha_02025_03_10_07_18_34/model_epoch_last.pth\n",
            "(Val @ epoch 31) acc: 0.99875; ap: 0.9999968769506866\n",
            "*************************\n",
            "2025_03_10_09_14_00\n",
            "(0 progan      ) acc: 99.1750; ap: 99.9832; r_acc: 0.9978; f_acc: 0.9858\n",
            "(1 stylegan    ) acc: 97.5000; ap: 99.9613; r_acc: 1.0000; f_acc: 0.9500\n",
            "(2 stylegan2   ) acc: 96.8125; ap: 99.6665; r_acc: 0.9988; f_acc: 0.9375\n",
            "(3 biggan      ) acc: 81.2500; ap: 91.6624; r_acc: 0.9050; f_acc: 0.7200\n",
            "(4 cyclegan    ) acc: 62.3113; ap: 88.5914; r_acc: 0.9792; f_acc: 0.2670\n",
            "(5 stargan     ) acc: 85.0000; ap: 96.0879; r_acc: 0.9700; f_acc: 0.7300\n",
            "(6 gaugan      ) acc: 58.0000; ap: 68.6272; r_acc: 0.8200; f_acc: 0.3400\n",
            "(7 deepfake    ) acc: 73.2500; ap: 84.7049; r_acc: 0.8900; f_acc: 0.5750\n",
            "(8 Mean      ) acc: 81.6624; ap: 91.1606\n",
            "*************************\n",
            "2025_03_10_09_15_01\n",
            "2025_03_10_09_15_21 Train loss/d_loss: 0.026047302410006523/1.682650089263916 at step: 12050 lr 0.00010628820000000004\n",
            "2025_03_10_09_15_41 Train loss/d_loss: 0.0008566373726353049/1.6725358963012695 at step: 12100 lr 0.00010628820000000004\n",
            "2025_03_10_09_16_00 Train loss/d_loss: 0.0006348444148898125/1.6396235227584839 at step: 12150 lr 0.00010628820000000004\n",
            "2025_03_10_09_16_20 Train loss/d_loss: 0.0017437799833714962/1.686400294303894 at step: 12200 lr 0.00010628820000000004\n",
            "2025_03_10_09_16_39 Train loss/d_loss: 0.0014294901629909873/1.6925759315490723 at step: 12250 lr 0.00010628820000000004\n",
            "2025_03_10_09_16_59 Train loss/d_loss: 0.010207860730588436/1.6736822128295898 at step: 12300 lr 0.00010628820000000004\n",
            "2025_03_10_09_17_19 Train loss/d_loss: 0.028439128771424294/1.7098984718322754 at step: 12350 lr 0.00010628820000000004\n",
            "Saving model ./checkpoints/adof_distill-alpha_02025_03_10_07_18_34/model_epoch_32.pth\n",
            " Epoch 32 : 2025_03_10_09_15_01 --> 2025_03_10_09_17_28\n",
            "Saving model ./checkpoints/adof_distill-alpha_02025_03_10_07_18_34/model_epoch_last.pth\n",
            "(Val @ epoch 32) acc: 0.9975; ap: 0.9999984394506867\n",
            "*************************\n",
            "2025_03_10_09_17_35\n",
            "(0 progan      ) acc: 99.5625; ap: 99.9793; r_acc: 0.9952; f_acc: 0.9960\n",
            "(1 stylegan    ) acc: 99.2500; ap: 99.9908; r_acc: 0.9983; f_acc: 0.9867\n",
            "(2 stylegan2   ) acc: 98.3125; ap: 99.7518; r_acc: 0.9938; f_acc: 0.9725\n",
            "(3 biggan      ) acc: 82.7500; ap: 89.0144; r_acc: 0.7000; f_acc: 0.9550\n",
            "(4 cyclegan    ) acc: 73.9151; ap: 87.5735; r_acc: 0.8877; f_acc: 0.5906\n",
            "(5 stargan     ) acc: 96.2500; ap: 99.1722; r_acc: 0.9600; f_acc: 0.9650\n",
            "(6 gaugan      ) acc: 67.5000; ap: 66.4788; r_acc: 0.5400; f_acc: 0.8100\n",
            "(7 deepfake    ) acc: 73.0000; ap: 85.2912; r_acc: 0.9050; f_acc: 0.5550\n",
            "(8 Mean      ) acc: 86.3175; ap: 90.9065\n",
            "*************************\n",
            "2025_03_10_09_18_37\n",
            "2025_03_10_09_18_47 Train loss/d_loss: 0.0011170434299856424/1.6707987785339355 at step: 12400 lr 0.00010628820000000004\n",
            "2025_03_10_09_19_07 Train loss/d_loss: 0.0002627460053190589/1.7175428867340088 at step: 12450 lr 0.00010628820000000004\n",
            "2025_03_10_09_19_27 Train loss/d_loss: 0.0005779778584837914/1.7005553245544434 at step: 12500 lr 0.00010628820000000004\n",
            "2025_03_10_09_19_46 Train loss/d_loss: 0.005358642898499966/1.6553117036819458 at step: 12550 lr 0.00010628820000000004\n",
            "2025_03_10_09_20_06 Train loss/d_loss: 0.009676843881607056/1.6377449035644531 at step: 12600 lr 0.00010628820000000004\n",
            "2025_03_10_09_20_25 Train loss/d_loss: 0.0002691131376195699/1.6894668340682983 at step: 12650 lr 0.00010628820000000004\n",
            "2025_03_10_09_20_45 Train loss/d_loss: 0.0047088805586099625/1.6413192749023438 at step: 12700 lr 0.00010628820000000004\n",
            "2025_03_10_09_21_04 Train loss/d_loss: 0.002058587037026882/1.70003342628479 at step: 12750 lr 0.00010628820000000004\n",
            "Saving model ./checkpoints/adof_distill-alpha_02025_03_10_07_18_34/model_epoch_33.pth\n",
            " Epoch 33 : 2025_03_10_09_18_37 --> 2025_03_10_09_21_04\n",
            "Saving model ./checkpoints/adof_distill-alpha_02025_03_10_07_18_34/model_epoch_last.pth\n",
            "(Val @ epoch 33) acc: 0.726875; ap: 0.9963881445814092\n",
            "*************************\n",
            "2025_03_10_09_21_11\n",
            "(0 progan      ) acc: 74.9250; ap: 99.2082; r_acc: 0.9995; f_acc: 0.4990\n",
            "(1 stylegan    ) acc: 69.0833; ap: 99.5668; r_acc: 1.0000; f_acc: 0.3817\n",
            "(2 stylegan2   ) acc: 72.9375; ap: 98.2253; r_acc: 1.0000; f_acc: 0.4587\n",
            "(3 biggan      ) acc: 65.5000; ap: 91.0208; r_acc: 0.9950; f_acc: 0.3150\n",
            "(4 cyclegan    ) acc: 57.3113; ap: 92.5640; r_acc: 1.0000; f_acc: 0.1462\n",
            "(5 stargan     ) acc: 63.0000; ap: 99.9659; r_acc: 1.0000; f_acc: 0.2600\n",
            "(6 gaugan      ) acc: 56.0000; ap: 85.3413; r_acc: 0.9900; f_acc: 0.1300\n",
            "(7 deepfake    ) acc: 51.0000; ap: 81.5286; r_acc: 1.0000; f_acc: 0.0200\n",
            "(8 Mean      ) acc: 63.7196; ap: 93.4276\n",
            "*************************\n",
            "2025_03_10_09_22_13\n",
            "2025_03_10_09_22_33 Train loss/d_loss: 0.00012164229701738805/1.7256300449371338 at step: 12800 lr 0.00010628820000000004\n",
            "2025_03_10_09_22_53 Train loss/d_loss: 0.0003008343046531081/1.628591537475586 at step: 12850 lr 0.00010628820000000004\n",
            "2025_03_10_09_23_12 Train loss/d_loss: 0.00019597966456785798/1.659011960029602 at step: 12900 lr 0.00010628820000000004\n",
            "2025_03_10_09_23_32 Train loss/d_loss: 0.0041410778649151325/1.6798888444900513 at step: 12950 lr 0.00010628820000000004\n",
            "2025_03_10_09_23_51 Train loss/d_loss: 0.0009939668234437704/1.7108019590377808 at step: 13000 lr 0.00010628820000000004\n",
            "2025_03_10_09_24_11 Train loss/d_loss: 0.01239512674510479/1.6406480073928833 at step: 13050 lr 0.00010628820000000004\n",
            "2025_03_10_09_24_31 Train loss/d_loss: 0.000138856063131243/1.6239091157913208 at step: 13100 lr 0.00010628820000000004\n",
            "Saving model ./checkpoints/adof_distill-alpha_02025_03_10_07_18_34/model_epoch_34.pth\n",
            " Epoch 34 : 2025_03_10_09_22_13 --> 2025_03_10_09_24_40\n",
            "Saving model ./checkpoints/adof_distill-alpha_02025_03_10_07_18_34/model_epoch_last.pth\n",
            "(Val @ epoch 34) acc: 0.935625; ap: 0.9997870061168596\n",
            "*************************\n",
            "2025_03_10_09_24_47\n",
            "(0 progan      ) acc: 90.2125; ap: 99.9180; r_acc: 0.9998; f_acc: 0.8045\n",
            "(1 stylegan    ) acc: 87.0000; ap: 99.9260; r_acc: 1.0000; f_acc: 0.7400\n",
            "(2 stylegan2   ) acc: 88.9375; ap: 99.3474; r_acc: 1.0000; f_acc: 0.7788\n",
            "(3 biggan      ) acc: 66.2500; ap: 90.2605; r_acc: 0.9900; f_acc: 0.3350\n",
            "(4 cyclegan    ) acc: 55.2830; ap: 91.8324; r_acc: 0.9981; f_acc: 0.1075\n",
            "(5 stargan     ) acc: 69.5000; ap: 98.2829; r_acc: 1.0000; f_acc: 0.3900\n",
            "(6 gaugan      ) acc: 55.2500; ap: 79.1317; r_acc: 0.9750; f_acc: 0.1300\n",
            "(7 deepfake    ) acc: 55.7500; ap: 81.3779; r_acc: 0.9850; f_acc: 0.1300\n",
            "(8 Mean      ) acc: 71.0229; ap: 92.5096\n",
            "*************************\n",
            "2025_03_10_09_25_51\n",
            "2025_03_10_09_26_01 Train loss/d_loss: 0.0002617292630020529/1.7020070552825928 at step: 13150 lr 0.00010628820000000004\n",
            "2025_03_10_09_26_21 Train loss/d_loss: 0.007422863971441984/1.6966936588287354 at step: 13200 lr 0.00010628820000000004\n"
          ]
        }
      ],
      "source": [
        "#experiment-01-no-filter\n",
        "backbone = 'adof_distill'\n",
        "!find $root_dir/datasets -type d -name \"*ipynb*\" -exec rm -r {} +\n",
        "!python train.py \\\n",
        "--name $backbone-alpha_0 \\\n",
        "--dataroot $root_dir/datasets/ForenSynths_train \\\n",
        "--num_thread 2 \\\n",
        "--classes car,cat,chair,horse --batch_size 64 --delr_freq 5 --loss_freq 50  --lr 0.0002 --niter 50 \\\n",
        "--backbone $backbone \\\n",
        "--gpu_ids 0 \\\n",
        "--use_comet"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": null,
      "metadata": {
        "id": "TVKOc8eJ5rS2"
      },
      "outputs": [],
      "source": [
        "#Continue Train\n",
        "backbone = 'adof'\n",
        "!find $root_dir/datasets -type d -name \"*ipynb*\" -exec rm -r {} +\n",
        "!python train.py \\\n",
        "--name adof-$backbone- \\\n",
        "--dataroot /workspace/datasets/ForenSynths_train \\\n",
        "--num_thread 8 \\\n",
        "--classes car,cat,chair,horse --batch_size 64 --delr_freq 5 --loss_freq 400  --lr 0.0002 --niter 30 \\\n",
        "--backbone $backbone \\\n",
        "--gpu_ids 0 \\\n",
        "--continue_train \\\n",
        "--epoch last \\\n",
        "--old_checkpoint checkpoints/adof-adof-2024_10_09_06_01_23/model_epoch_last.pth\n",
        ""
      ]
    },
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "KFlXRww3Pk8G"
      },
      "source": [
        "## Download Data"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": 6,
      "metadata": {
        "id": "MHqGlQRbQNE3",
        "outputId": "aed1f9a6-33a4-4ac4-d131-f3b2f2e8b68d",
        "colab": {
          "base_uri": "https://localhost:8080/"
        }
      },
      "outputs": [
        {
          "output_type": "stream",
          "name": "stderr",
          "text": [
            "Downloading...\n",
            "From (original): https://drive.google.com/uc?id=1JLdFyM7JnaUa8y4wBOwNGs3rn4qKH58Z\n",
            "From (redirected): https://drive.google.com/uc?id=1JLdFyM7JnaUa8y4wBOwNGs3rn4qKH58Z&confirm=t&uuid=7ab2b1f4-1bd2-4c42-a976-defa5a049637\n",
            "To: /content/Progan_train.zip\n",
            "100%|██████████| 2.64G/2.64G [00:18<00:00, 142MB/s]\n"
          ]
        }
      ],
      "source": [
        "#Download Train, val sethttps://drive.google.com/file/d/1JLdFyM7JnaUa8y4wBOwNGs3rn4qKH58Z/view?usp=drive_link\n",
        "import gdown\n",
        "file_id = '1JLdFyM7JnaUa8y4wBOwNGs3rn4qKH58Z' #Progan train/val 4 class [car, cat, chair, horse] 13Gb\n",
        "#file_id = '1cyGbozooewAt6pNH9gw2_sDMM-Locqh2' #Progan train/val 4 class [car, cat, chair, horse]\n",
        "destination = root_dir + '/Progan_train.zip'  # Desired file name and extension\n",
        "# Construct the download URL\n",
        "url = f'https://drive.google.com/uc?id={file_id}'\n",
        "# Download the file\n",
        "gdown.download(url, destination, quiet=False)\n",
        "#Unzip\n",
        "!mkdir $root_dir/datasets\n",
        "!unzip -q $destination -d $root_dir/datasets/ForenSynths_train"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": 7,
      "metadata": {
        "id": "YEJ2C7IoQ5co",
        "outputId": "84d29f18-7b2e-42ee-9d6b-2e5f94dfce55",
        "colab": {
          "base_uri": "https://localhost:8080/"
        }
      },
      "outputs": [
        {
          "output_type": "stream",
          "name": "stderr",
          "text": [
            "Downloading...\n",
            "From (original): https://drive.google.com/uc?id=1-02FPvdTaQFDMatEvXAeKLCmnwW6RLx4\n",
            "From (redirected): https://drive.google.com/uc?id=1-02FPvdTaQFDMatEvXAeKLCmnwW6RLx4&confirm=t&uuid=8b5973c7-ca8e-4efe-922a-247ed65b235f\n",
            "To: /content/test_set.zip\n",
            "100%|██████████| 1.68G/1.68G [00:16<00:00, 104MB/s] \n"
          ]
        }
      ],
      "source": [
        "#Download small Test set (this is a small testset just use for reference during training)\n",
        "import gdown\n",
        "file_id = '1-02FPvdTaQFDMatEvXAeKLCmnwW6RLx4' #test_set 1.57Gb\n",
        "destination = root_dir + '/test_set.zip'  # Desired file name and extension\n",
        "# Construct the download URL\n",
        "url = f'https://drive.google.com/uc?id={file_id}'\n",
        "# Download the file\n",
        "gdown.download(url, destination, quiet=False)\n",
        "#Unzip\n",
        "!mkdir -p $root_dir/datasets/ForenSynths_train\n",
        "!unzip -q $destination -d $root_dir/datasets/ForenSynths_train/test"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": null,
      "metadata": {
        "id": "ECdLkhQR5rS3"
      },
      "outputs": [],
      "source": []
    }
  ],
  "metadata": {
    "accelerator": "GPU",
    "colab": {
      "collapsed_sections": [
        "ZiHwX8vIN8xm"
      ],
      "gpuType": "T4",
      "provenance": []
    },
    "kernelspec": {
      "display_name": "Python 3 (Spyder)",
      "language": "python3",
      "name": "python3"
    },
    "language_info": {
      "codemirror_mode": {
        "name": "ipython",
        "version": 3
      },
      "file_extension": ".py",
      "mimetype": "text/x-python",
      "name": "python",
      "nbconvert_exporter": "python",
      "pygments_lexer": "ipython3",
      "version": "3.10.13"
    }
  },
  "nbformat": 4,
  "nbformat_minor": 0
}